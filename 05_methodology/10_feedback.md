# Stage 10: Feedback
_"By collecting results from the implemented model, the organization gets feedback on the modelâ€™s performance and its impact on the environment in which it was deployed. For example, feedback could take the form of response rates to a promotional campaign targeting a group of customers identified by the model as high-potential responders. Analyzing this feedback enables data scientists to refine the model to improve its accuracy and usefulness. They can automate some or all of the feedback-gathering and model assessment, refinement and redeployment steps to speed up the process of model refreshing for better outcomes."_ - **John B. Rollins**


## Purpose
The goal of this final stage is to close the data science lifecycle loop. We will monitor the deployed model's real-world performance, measure its actual business impact, and collect feedback to guide future iterations. A model in production is not a static artifact; it is a product that requires ongoing maintenance and evaluation to ensure it continues to deliver value.


## Step 1: Establish Monitoring & Logging
Implement systems to capture the necessary data to evaluate the live model's performance and health.

* **Action:** Set up automated logging for model inputs, predictions, and system performance.
* **Guiding Questions:**
    * How will we log incoming prediction requests and the model's outputs?
    * How will we monitor technical metrics like prediction latency and system uptime?
    * How will we detect "data drift" (when production data's statistical properties change from the training data)?
* **MLOps Connection:** This step puts the concepts from `04_mlops/06_ml_monitoring_concepts/` into practice.

> **Monitoring & Logging Setup:**
>
> * **Tools Used:** *[Example: Custom logging to a PostgreSQL database, with Grafana for dashboards.]*
> * **Key Metrics Tracked:** *[Example: API latency (p95), prediction volume, input data distribution via statistical tests.]*


## Step 2: Collect Production Data and True Outcomes
Set up a reliable pipeline to gather the model's live predictions and, crucially, to match them with the actual, true outcomes as they become available.

* **Action:** Design and implement the feedback data pipeline.
* **Guiding Questions:**
    * How long after a prediction is made will the true outcome be known (e.g., does a customer actually churn 30 days later)?
    * What process will join the predictions with the actual outcomes?

> **Feedback Data Pipeline:**
>
> * **Description:** *[Example: "A nightly batch job runs to join the prediction logs with the customer subscription status table from the CRM. The results are stored in a dedicated analytics table."]*
> * **Data Location:** *[Example: `analytics_db.public.model_feedback_logs`]*


## Step 3: Analyze Model Performance in Production
Periodically recalculate the key technical metrics using the live production data and compare them against the test set results from Stage 8.

* **Action:** Create a dashboard or report to track live model performance over time.
* **Guiding Question:** Is the live performance consistent with the expected performance? Is it degrading over time?

> **Live Performance Tracking:**
>
> | Date | Metric (AUC) | Value | Comment |
> | :--- | :--- | :--- | :--- |
> | *2025-06-20*| *Test Set* | *0.84* | *Baseline performance from Stage 8.* |
> | *2025-07-31*| *Live Data*| *0.83* | *Performance is stable and consistent with test set.* |
> | *2025-08-31*| *Live Data*| *0.82* | *Slight dip, within expected variance.* |
> | *2025-09-30*| *Live Data*| *0.77* | **Alert:** *Performance has dropped significantly. Investigation required.* |


## Step 4: Analyze Business Impact
Compare the actual business impact with the projected impact from Stage 8.

* **Action:** Work with business stakeholders to quantify the value generated by the model.

> **Business Impact Summary (Q3 2025):**
>
> * *[Example: "The model was used to drive two retention campaigns in Q3. The targeted group showed a 4.5% lower churn rate than the control group, resulting in an estimated $250,000 in retained revenue for the quarter. This is in line with the project's primary objective."]*


## Step 5: Review and Plan Next Iteration
Based on the performance and business analysis, decide on the future of the current model version.

* **Action:** Hold a periodic review meeting to decide on the next steps.

> **Decision for Next Cycle:**
>
> * *(Choose one)*
>   * [ ] **No Action Needed:** The model is performing as expected.
>   * [X] **Retrain Model:** Performance has degraded due to data drift. A simple retraining on newer data is required.
>   * [ ] **Rebuild Model:** The underlying business problem or data patterns have fundamentally changed. A new analytic approach is needed (return to Stage 2).
>   * [ ] **Decommission Model:** The model is no longer needed or providing value.
>
> * **Justification:** *[Example: "Performance degraded in September (see Step 3) due to a recent marketing campaign that changed customer behavior. We will retrain the model including data from the last quarter."]*
> * **Next Cycle Start Date:** *[e.g., 2025-10-01]*


## Step 6: Final Documentation
Conclude the current project lifecycle by archiving the results.

* **Action:** Finalize and archive the performance and business impact reports for this model version.
* **Action:** Add a summary of this stage's final business impact to the main project `README.md`.