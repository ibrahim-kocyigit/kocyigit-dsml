### **1. Differential Calculus (Core Focus)**
   - **Derivatives & Differentiation Rules**  
     - Power rule, product rule, chain rule, quotient rule  
     - Partial derivatives (for multivariable functions)  
   - **Gradients** (∇) – Critical for optimization (e.g., gradient descent)  
   - **Higher-Order Derivatives** (e.g., Hessian matrix in second-order optimization)  
   - **Common Functions Used in ML**  
     - Exponential, logarithmic, and trigonometric derivatives  
     - Derivatives of activation functions (Sigmoid, ReLU, Tanh, Softmax)  

### **2. Integral Calculus (Less Emphasis, but Useful)**
   - **Basic Integration** (Definite & Indefinite integrals)  
   - **Probability & Statistics Applications**  
     - Calculating expectations, variance (e.g., in Bayesian ML)  
   - **Numerical Integration** (Monte Carlo methods for approximations)  

### **3. Multivariable Calculus (Very Important)**
   - **Partial Derivatives** (for functions of multiple variables)  
   - **Gradient Vector & Directional Derivatives** (used in optimization)  
   - **Jacobian Matrix** (for vector-valued functions, used in backpropagation)  
   - **Hessian Matrix** (second-order derivatives, used in advanced optimization)  

### **4. Optimization (Key for ML)**
   - **Gradient Descent & Variants** (SGD, Adam, RMSprop)  
   - **Convex vs. Non-Convex Optimization** (Loss landscapes in neural networks)  
   - **Lagrange Multipliers** (Used in constrained optimization, e.g., SVMs)  

### **5. Applications in Machine Learning**
   - **Backpropagation** (Relies heavily on the chain rule)  
   - **Loss Function Optimization** (MSE, Cross-Entropy, etc.)  
   - **Regularization Techniques** (L1/L2 regularization involve derivatives)  
