{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f5ef14-4c4b-4347-b6c7-089bc83be6a7",
   "metadata": {},
   "source": [
    "# Systems of Equations and Vector/Matrix Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1fd70-041d-476b-89a6-65cd90965ebe",
   "metadata": {},
   "source": [
    "In the previous section, we established that a linear regression model with $n$ features and $m$ data records can be represented as a system of linear equations:\n",
    "\n",
    "* **Record 1**: $w_1x_1^{(1)} + w_2x_2^{(1)} + \\dots + w_nx_n^{(1)} + b = y^{(1)}$\n",
    "* **Record 2**: $w_1x_1^{(2)} + w_2x_2^{(2)} + \\dots + w_nx_n^{(2)} + b = y^{(2)}$\n",
    "* ...\n",
    "* **Record m**: $w_1x_1^{(m)} + w_2x_2^{(m)} + \\dots + w_nx_n^{(m)} + b = y^{(m)}$\n",
    "\n",
    "An important thing to remember is that the **weights ($w$)** and the **bias ($b$)** are the **same for all equations**. We are looking for a single set of these values that works for every record in our dataset. The features ($x$) and targets ($y$) are unique to each row.\n",
    "\n",
    "---\n",
    "\n",
    "## A More Compact Notation: Vectors and Matrices\n",
    "\n",
    "Writing out the full system of equations can be cumbersome. Linear algebra gives us a much more efficient way to represent this using **vectors** and **matrices**.\n",
    "\n",
    "For now, you can think of them simply as:\n",
    "* **Vector**: A list of numbers.\n",
    "* **Matrix**: A grid of numbers (a collection of vectors).\n",
    "\n",
    "Using this, we can simplify our entire system of equations into a single, elegant line:\n",
    "\n",
    "$$ y = WX + b $$\n",
    "\n",
    "* $W$ is now a **vector** containing all the weights ($w_1, w_2, \\dots, w_n$).\n",
    "* $X$ is a **matrix** where each row contains the features for a single data record.\n",
    "* $b$ is the bias term.\n",
    "* $y$ is a **vector** of all the target outputs ($y^{(1)}, y^{(2)}, \\dots, y^{(m)}$).\n",
    "\n",
    "This compact form is the backbone of how we perform calculations in machine learning.\n",
    "\n",
    "*(Note: If you're already familiar with linear algebra, you might notice we're being a bit imprecise here. Depending on how we define our vectors and matrices, we might need to use a transpose ($W^T$) to make the math work. We'll ignore that detail for now to focus on the core concept.)*\n",
    "\n",
    "--- \n",
    "\n",
    "## Analytical vs. Empirical Solutions\n",
    "\n",
    "When you have a system of linear equations, there are two general ways to \"solve\" it.\n",
    "\n",
    "* **Analytical Solution**: This is what you do with a pencil and paper. Using algebraic methods like substitution or elimination, you can find the *exact* values for your unknowns. This is possible if you have a \"perfect\" system, often where the number of unique equations matches the number of unknown variables.\n",
    "\n",
    "* **Empirical Solution**: This is the machine learning approach. In reality, data is noisy and a perfect solution rarely exists. Linear regression finds the best *approximate* solution by iteratively adjusting the weights and bias to minimize the error. It's an empirical, or data-driven, way of finding the best-fit line or plane.\n",
    "\n",
    "---\n",
    "\n",
    "## A New Problem: What Are Your Scores? ðŸ¤”\n",
    "\n",
    "Let's frame a new problem to explore these concepts. Imagine you took three courses: **Linear Algebra**, **Calculus**, and **Probability & Statistics**. I know your scores, but I won't tell you directly. Instead, I'll give you these three facts:\n",
    "\n",
    "1.  Your Linear Algebra score plus your Calculus score minus your Probability & Statistics score is **6**.\n",
    "2.  Your Linear Algebra score minus your Calculus score plus double your Probability & Statistics score is **4**.\n",
    "3.  Four times your Linear Algebra score minus double your Calculus score plus your Probability & Statistics score is **10**.\n",
    "\n",
    "Can we use this information to figure out your exact scores? Let's turn these sentences into a system of linear equations.\n",
    "\n",
    "--- \n",
    "\n",
    "## Formulating the System of Equations\n",
    "\n",
    "Let's assign variables to our unknown scores:\n",
    "* $a$ = Linear Algebra score\n",
    "* $c$ = Calculus score\n",
    "* $p$ = Probability & Statistics score\n",
    "\n",
    "Now we can translate the sentences into equations:\n",
    "\n",
    "1.  $a + c - p = 6$\n",
    "2.  $a - c + 2p = 4$\n",
    "3.  $4a - 2c + p = 10$\n",
    "\n",
    "This is a classic system of linear equations. Since we have 3 distinct equations and 3 unknowns, we might be able to solve this analytically (We will cover how to solve systems of linear equations later).\n",
    "\n",
    "---\n",
    "\n",
    "## Relating This Back to Machine Learning\n",
    "\n",
    "Our course-score problem is a perfect analogy for the components of a machine learning model. In linear algebra, we represent these components efficiently using **vectors** (a column of numbers) and **matrices** (a grid of numbers). This allows us to write a complex system of equations in a very simple form.\n",
    "\n",
    "### Weights (The Unknowns) - Vector $w$\n",
    "\n",
    "The **weights** are the unknown values we are trying to solve for. In machine learning, these are the parameters the model \"learns.\"\n",
    "\n",
    "* In our problem, the unknowns are the scores: $a, c, p$.\n",
    "* We collect these into a single column **vector**:\n",
    "$\n",
    "w = \\begin{bmatrix} a \\\\ c \\\\ p \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Features (The Known Coefficients) - Matrix $X$\n",
    "\n",
    "The **features** are the known coefficients that multiply each of our unknown weights. In a real dataset, these would be your input data points.\n",
    "\n",
    "* We collect all the features from our system of equations into a **matrix**, which we can call $X$. Each row in the matrix corresponds to one of our equations: \n",
    "$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1 & -1 \\\\\n",
    "1 & -1 & 2 \\\\\n",
    "4 & -2 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Targets (The Known Outcomes) - Vector $y$\n",
    "\n",
    "The **targets** are the known results on the other side of the equal sign. In a dataset, this would be the \"correct answer\" for each row of features.\n",
    "\n",
    "* In our problem, the targets are the equation outcomes: 6, 4, 10.\n",
    "* We collect these into a column **vector**, which we can call: \n",
    "$\n",
    "y = \\begin{bmatrix} 6 \\\\ 4 \\\\ 10 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### The Full Picture: $Xw = y$\n",
    "\n",
    "When we put it all together, our entire system of three separate equations can be written in a single, compact matrix-vector equation: **$Xw = y$**.\n",
    "\n",
    "Visually, that looks like this:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & -1 \\\\\n",
    "1 & -1 & 2 \\\\\n",
    "4 & -2 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} a \\\\ c \\\\ p \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 6 \\\\ 4 \\\\ 10 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This elegant notation is fundamental to linear algebra and is used everywhere in machine learning to represent data and solve complex problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
