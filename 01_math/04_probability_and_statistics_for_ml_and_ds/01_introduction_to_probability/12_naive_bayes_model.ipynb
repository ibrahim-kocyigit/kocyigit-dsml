{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185eb73d-d93e-4808-aa04-8a3c0e75fbea",
   "metadata": {},
   "source": [
    "# Bayes' Theorem - The Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d631f3e5-23b7-4176-9170-dbd51f5d16ab",
   "metadata": {},
   "source": [
    "In the previous lesson, we used Bayes' theorem to build a simple spam filter based on the presence of a single word, \"lottery.\" But what happens when we want to use multiple words, like \"lottery\" and \"winning,\" to make a better prediction?\n",
    "\n",
    "The direct approach would be to calculate:\n",
    "$$ P(\\text{Spam} | \\text{\"lottery\" AND \"winning\"}) $$\n",
    "To do this, we would need to find the number of spam emails that contain *both* words and the total number of emails that contain *both* words. For 100 words, we might not have any emails in our dataset that contain all 100, making this calculation impossible.\n",
    "\n",
    "This is where the **Naive Bayes** model comes in. It solves this problem by making a \"naive,\" but very powerful, assumption.\n",
    "\n",
    "---\n",
    "## The Naive Assumption: Feature Independence\n",
    "\n",
    "The core idea of the Naive Bayes classifier is to **assume that all features (in this case, the presence of words) are independent of each other.**\n",
    "\n",
    "This means we assume that the probability of seeing the word \"winning\" is not affected by whether the word \"lottery\" is also present. In reality, this is not true (words are often dependent), but this simplifying assumption makes the math much easier and, surprisingly, often leads to a very effective model.\n",
    "\n",
    "Because we assume independence, we can use the **product rule**. The probability of seeing both \"lottery\" AND \"winning\" in a spam email is simply the product of their individual probabilities:\n",
    "$$ P(\\text{lottery} \\cap \\text{winning} | \\text{Spam}) \\approx P(\\text{lottery} | \\text{Spam}) \\cdot P(\\text{winning} | \\text{Spam}) $$\n",
    "\n",
    "This allows us to calculate the probability of seeing many words together without needing to find a single email that contains all of them.\n",
    "\n",
    "---\n",
    "## A Worked Example\n",
    "\n",
    "Let's use this assumption to calculate $P(\\text{Spam} | \\text{\"lottery\" AND \"winning\"})$ using the following probabilities:\n",
    "\n",
    "**1. Priors (from the previous lesson):**\n",
    "* $P(\\text{Spam}) = 0.2$\n",
    "* $P(\\text{Ham}) = 0.8$\n",
    "\n",
    "**2. Likelihoods for \"lottery\":**\n",
    "* $P(\\text{lottery} | \\text{Spam}) = \\frac{14}{20} = 0.7$\n",
    "* $P(\\text{lottery} | \\text{Ham}) = \\frac{10}{80} = 0.125$\n",
    "\n",
    "**3. Likelihoods for \"winning\":**\n",
    "* $P(\\text{winning} | \\text{Spam}) = \\frac{15}{20} = 0.75$\n",
    "* $P(\\text{winning} | \\text{Ham}) = \\frac{8}{80} = 0.1$\n",
    "\n",
    "Now, we apply Bayes' theorem, but we use our naive assumption to calculate the likelihood of seeing both words.\n",
    "\n",
    "* **Numerator:** $P(\\text{Spam}) \\cdot P(\\text{lottery} \\cap \\text{winning} | \\text{Spam})$\n",
    "  $$ \\approx P(\\text{Spam}) \\cdot P(\\text{lottery} | \\text{Spam}) \\cdot P(\\text{winning} | \\text{Spam}) $$\n",
    "  $$ = 0.2 \\times 0.7 \\times 0.75 = 0.105 $$\n",
    "\n",
    "* **Denominator:** (The total probability of seeing both words)\n",
    "  $$ \\approx [P(\\text{Spam}) \\cdot P(\\text{lottery}|S) \\cdot P(\\text{winning}|S)] + [P(\\text{Ham}) \\cdot P(\\text{lottery}|H) \\cdot P(\\text{winning}|H)] $$\n",
    "  $$ = (0.2 \\cdot 0.7 \\cdot 0.75) + (0.8 \\cdot 0.125 \\cdot 0.1) $$\n",
    "  $$ = 0.105 + 0.01 = 0.115 $$\n",
    "\n",
    "* **Final Posterior Probability:**\n",
    "  $$ P(\\text{Spam} | \\text{lottery} \\cap \\text{winning}) = \\frac{\\text{Numerator}}{\\text{Denominator}} = \\frac{0.105}{0.115} \\approx 0.913 $$\n",
    "\n",
    "By combining the evidence from two words, our belief that the email is spam has increased dramatically, from 20% to **91.3%**. This is the power of the Naive Bayes algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
