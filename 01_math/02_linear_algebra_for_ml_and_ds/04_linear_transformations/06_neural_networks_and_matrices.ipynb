{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88e01d1-ad9e-42c2-94b3-f51b96f1177f",
   "metadata": {},
   "source": [
    "# Neural Networks and Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c411e1-94f7-4336-9de4-af0c30f3f204",
   "metadata": {},
   "source": [
    "Neural networks, one of the most powerful models in machine learning, are built almost entirely on the concepts of matrices and vector operations that we've been studying. In this lesson, we'll see a concrete example of how a simple neural network works using the dot product.\n",
    "\n",
    "---\n",
    "## A Spam Detection Problem\n",
    "\n",
    "Let's imagine we have a dataset of emails, and we've identified two words that seem important for detecting spam: \"lottery\" and \"win.\" We've counted the number of times these words appear in a few sample emails.\n",
    "\n",
    "| # of \"lottery\" | # of \"win\" | Is it Spam? |\n",
    "| :---: | :---: | :---: |\n",
    "| 1 | 1 | Yes |\n",
    "| 2 | 1 | Yes |\n",
    "| 0 | 0 | No |\n",
    "| 0 | 2 | Yes |\n",
    "| 0 | 1 | No |\n",
    "| 1 | 0 | No |\n",
    "| 2 | 2 | Yes |\n",
    "| 2 | 0 | Yes |\n",
    "| 1 | 2 | Yes |\n",
    "\n",
    "Our goal is to build a **classifier**—a simple machine that can guess if a new email is spam based on the counts of these two words.\n",
    "\n",
    "---\n",
    "## The Classifier's Logic\n",
    "\n",
    "Our classifier will work in two steps:\n",
    "\n",
    "1.  **Calculate a Score:** We will assign a \"weight\" (a score) to each word. The total score for an email is the sum of the weights of the words it contains.\n",
    "    * `Total Score = (weight_lottery * count_lottery) + (weight_win * count_win)`\n",
    "\n",
    "2.  **Apply a Threshold:** We will choose a \"threshold\" value.\n",
    "    * If `Total Score > Threshold`, we classify the email as **Spam**.\n",
    "    * Otherwise, we classify it as **Not Spam**.\n",
    "\n",
    "The \"learning\" task is to find the best weights and the best threshold to make our classifier's guesses match the \"Is it Spam?\" column in our table as closely as possible.\n",
    "\n",
    "For this dataset, there are many perfect solutions. For example, if we choose the following values...\n",
    "\n",
    "* `weight_lottery = 1`\n",
    "* `weight_win = 1`\n",
    "* `Threshold = 1.5`\n",
    "\n",
    "... our classifier correctly identifies every email in our table.\n",
    "\n",
    "---\n",
    "## A Geometric View: The Linear Classifier\n",
    "\n",
    "We can visualize our dataset on a 2D plane, where the axes represent the counts of our two words. The rule `1*lottery + 1*win > 1.5` creates a line that perfectly separates the \"Spam\" points from the \"Not Spam\" points. This is a **linear classifier**, and it's the simplest form of a neural network: a one-layer neural network, also known as a **perceptron**.\n",
    "\n",
    "![](./images/0601.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9a6c2-edd9-4560-bf7c-47e039af627a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The One-Layer Neural Network as a Matrix Product\n",
    "\n",
    "The process of calculating the score for every email at once can be expressed as a single **matrix-vector product**.\n",
    "\n",
    "* The **Data Matrix (X)** contains the features for all our emails.\n",
    "* The **Model Vector (w)** contains the weights for our words.\n",
    "\n",
    "$$\n",
    "\\text{Data Matrix (X)} = \\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "2 & 1 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 2 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & 2 \\\\\n",
    "2 & 0 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\text{Model Vector (w)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The product $Xw$ gives us a vector of the total scores for each email:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "2 & 1 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 2 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & 2 \\\\\n",
    "2 & 0 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 2 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\\\ 1 \\\\ 4 \\\\ 2 \\\\ 3 \\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "We can then apply our threshold check (`> 1.5`) to this score vector to get our final predictions.\n",
    "\n",
    "---\n",
    "## The Bias Trick\n",
    "\n",
    "Instead of using a separate threshold, it's more common in neural networks to incorporate it into the model as a **bias** term.\n",
    "\n",
    "The check `score > 1.5` is equivalent to `score - 1.5 > 0`. This `-1.5` is our bias.\n",
    "\n",
    "We can include the bias in our matrix multiplication by adding a column of `1`s to our data matrix and adding the bias term to our model vector. The check then simplifies to just checking if the result is positive:\n",
    "\n",
    "**1. Augment the Data Matrix (X):** We add a column of ones.  \n",
    "\n",
    "$$\n",
    "X_{aug} = \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "2 & 1 & 1 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "2 & 2 & 1 \\\\\n",
    "2 & 0 & 1 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "**2. Augment the Model Vector (w):** We add the bias term.  \n",
    "\n",
    "$$\n",
    "w_{aug} = \\begin{bmatrix} 1 \\\\ 1 \\\\ -1.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**3. Perform the Matrix-Vector Multiplication:**\n",
    "\n",
    "$$\n",
    "X_{aug} \\cdot w_{aug} = \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "2 & 1 & 1 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "2 & 2 & 1 \\\\\n",
    "2 & 0 & 1 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ -1.5 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 0.5 \\\\ 1.5 \\\\ -1.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\\\ 2.5 \\\\ 0.5 \\\\ 1.5 \\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "Now, we just need to check if these results are greater than 0. This gives us the exact same predictions as checking if the original scores were greater than 1.5. This \"bias trick\" is the standard way neural networks handle thresholds.\n",
    "\n",
    "---\n",
    "## The AND Operator: Another Linearly Separable Problem\n",
    "\n",
    "This same simple neural network structure can be used to model basic logical operations. Let's look at the **AND** operator. The AND operator is true (outputs 1 or \"Yes\") only when both its inputs are true (1 or \"Yes\"), and false (outputs 0 or \"No\") otherwise:\n",
    "\n",
    "| x | y | x AND y |\n",
    "|:-:|:-:|:-------:|\n",
    "| 0 | 0 | 0 (No)  |\n",
    "| 1 | 0 | 0 (No)  |\n",
    "| 0 | 1 | 0 (No)  |\n",
    "| 1 | 1 | 1 (Yes) |\n",
    "\n",
    "We can use a perceptron to model this. Let's use the same weights as before (`weight_x = 1`, `weight_y = 1`) and adjust the bias.\n",
    "\n",
    "Our score will be: `score = (1 * x) + (1 * y)`\n",
    "\n",
    "Let's try a bias of `-1.5`. Our condition for \"AND is True\" becomes:\n",
    "`score - 1.5 > 0`  or  `x + y - 1.5 > 0`\n",
    "\n",
    "Let's see how this works for our dataset:\n",
    "\n",
    "* **x=0, y=0:** `0 + 0 - 1.5 = -1.5` (Not > 0) -> **No** (Correct)\n",
    "* **x=1, y=0:** `1 + 0 - 1.5 = -0.5` (Not > 0) -> **No** (Correct)\n",
    "* **x=0, y=1:** `0 + 1 - 1.5 = -0.5` (Not > 0) -> **No** (Correct)\n",
    "* **x=1, y=1:** `1 + 1 - 1.5 = 0.5` (> 0) -> **Yes** (Correct)\n",
    "\n",
    "This simple perceptron with the weights `w = [1, 1]` and a bias `b = -1.5` correctly implements the AND operator.\n",
    "\n",
    "You can visualize this as a line separating the points (0,0), (1,0), and (0,1) from the point (1,1) on a 2D plane. The equation of the line is $x + y - 1.5 = 0$.\n",
    "\n",
    "You can also represent this perceptron graphically as a node that takes inputs x and y, multiplies them by their respective weights, adds the bias, and then applies an activation function (in this case, checking if the result is greater than zero).\n",
    "\n",
    "![Perceptron 1](./images/0602.png)\n",
    "\n",
    "---\n",
    "\n",
    "### The Perceptron Diagram\n",
    "\n",
    "The diagram below illustrates the structure of a simple perceptron. The inputs (features `x₁` and `x₂`) are fed into the main node, each multiplied by its corresponding weight (`w₁` and `w₂`). Inside the node, this weighted sum is added to a bias term (`b`). The result is then passed through an **activation function**, which acts as a decision-maker to produce the final output—a classification of either **1** (e.g., \"Spam\", \"Will Churn\", \"Yes\") or **0** (e.g., \"Not Spam\", \"Will Not Churn\", \"No\").\n",
    "\n",
    "![Perceptron 1](./images/0603.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
