{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50491e71-7ac8-47d1-a424-6f2aaddb3967",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) - An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd0ab6-777d-476b-97f0-6a81b39d3581",
   "metadata": {},
   "source": [
    "Now, let's put all the pieces together to understand how Principal Component Analysis (PCA) works.\n",
    "\n",
    "Recall our goal: to take a dataset and project it onto a line (or a lower-dimensional plane) that preserves the most information. We learned that the \"best\" line is the one that **maximizes the variance** (the spread) of the projected data.\n",
    "\n",
    "The big question is: **How do we find this best line?**\n",
    "\n",
    "This is where all the concepts we've been studying—projections, eigenvectors, and the covariance matrix—come together in a clever way.\n",
    "\n",
    "---\n",
    "\n",
    "## The PCA Algorithm\n",
    "\n",
    "Here are the high-level steps for performing PCA on a 2D dataset.\n",
    "\n",
    "### Step 1: Center the Data\n",
    "First, we calculate the mean of our dataset and shift the data so that its center is at the origin (0, 0).\n",
    "\n",
    "### Step 2: Calculate the Covariance Matrix\n",
    "Next, we calculate the **covariance matrix** for our centered data. This 2x2 matrix, `C`, compactly represents the spread (variance) and relationship (covariance) between our two features.\n",
    "\n",
    "Suppose for our data, we calculate the following covariance matrix:  \n",
    "\n",
    "$\n",
    "C = \\begin{bmatrix}\n",
    "9 & 4 \\\\\n",
    "4 & 3\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Step 3: Find the Eigenvectors and Eigenvalues of the Covariance Matrix\n",
    "This is the most important leap in the process. The eigenvectors of the covariance matrix point in the directions of maximum variance in the data. The corresponding eigenvalues tell us how *much* variance is in each of those directions.\n",
    "\n",
    "For our covariance matrix `C`, we would find:\n",
    "* **Eigenvector 1:** $v_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$ with **Eigenvalue 1:** $\\lambda_1 = 11$\n",
    "\n",
    "* **Eigenvector 2:** $v_2 = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$ with **Eigenvalue 2:** $\\lambda_2 = 1$\n",
    "\n",
    "### Step 4: Select the Principal Component\n",
    "The eigenvector with the **largest eigenvalue** is the direction that captures the most variance. This is our **first principal component**.\n",
    "\n",
    "In our case, $\\lambda_1 = 11$ is much larger than $\\lambda_2 = 1$, so our principal component is the direction defined by the eigenvector $v_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$. This is the \"best line\" we've been looking for. We can discard the second eigenvector.\n",
    "\n",
    "### Step 5: Project the Data\n",
    "The final step is to project our original data onto the line spanned by our chosen principal component. The result is a new, 1-dimensional dataset that has preserved the maximum possible variance.\n",
    "\n",
    "![](./images/0501.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c922ae8-3240-459e-90db-075c6bacf295",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PCA for a High-Dimensional Dataset\n",
    "\n",
    "This process works for datasets of any size. Imagine you have a dataset with 9 features (a 9-dimensional dataset).\n",
    "\n",
    "1.  **Calculate the 9x9 Covariance Matrix.**\n",
    "2.  **Find the 9 Eigenvalues and their corresponding Eigenvectors.**\n",
    "3.  **Sort the eigenvectors** by their eigenvalues, from largest to smallest.\n",
    "4.  **Choose the top *k* eigenvectors.** If you want to reduce your dataset to 2 dimensions, you keep the two eigenvectors with the two largest eigenvalues and discard the rest.\n",
    "5.  **Project the data.** Create a new matrix `V` where the columns are your chosen eigenvectors (normalized to have a length of 1). The final, reduced dataset is calculated as `A_projected = A_original · V`.\n",
    "\n",
    "The result is a new dataset with the same number of rows but only *k* columns, having lost as little information as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
