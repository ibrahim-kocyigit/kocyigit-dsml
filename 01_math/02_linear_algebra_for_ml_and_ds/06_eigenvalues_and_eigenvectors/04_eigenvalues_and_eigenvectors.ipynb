{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083519b9-3fca-4d60-b8af-384124a900a3",
   "metadata": {},
   "source": [
    "# Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84d0b0-b3d4-45db-9770-3c1e06728b6c",
   "metadata": {},
   "source": [
    "Let's look more closely at why **eigenvectors** are so special. They are vectors whose direction is unchanged by a linear transformation; they are only scaled (stretched or shrunk).\n",
    "\n",
    "Consider our transformation matrix:  \n",
    "\n",
    "$ A = \\begin{bmatrix} 2 & 1 \\\\ 0 & 3 \\end{bmatrix} $\n",
    "\n",
    "Let's see how it acts on three different vectors:  \n",
    "\n",
    "* **Vector 1 (Eigenvector):** $v_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$  \n",
    "\n",
    "   $ A \\cdot v_1 = \\begin{bmatrix} 2 & 1 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} $  \n",
    "\n",
    "   The result is $2 \\cdot v_1$. The vector stayed on the same line.\n",
    "\n",
    "* **Vector 2 (Eigenvector):** $v_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$  \n",
    "\n",
    "   $ A \\cdot v_2 = \\begin{bmatrix} 2 & 1 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} $  \n",
    "\n",
    "   The result is $3 \\cdot v_2$. The vector stayed on the same line.\n",
    "\n",
    "* **Vector 3 (Not an Eigenvector):** $v_3 = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$  \n",
    "\n",
    "   $ A \\cdot v_3 = \\begin{bmatrix} 2 & 1 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 6 \\end{bmatrix} $  \n",
    "\n",
    "   The resulting vector points in a completely different direction.\n",
    "\n",
    "![](./images/0401.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db1641-8390-4ee4-b6a7-24a579894a8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Formal Definition\n",
    "\n",
    "This special relationship is captured by the core equation of eigenvectors and eigenvalues. For a given matrix `A`, a vector `v` is an eigenvector if it satisfies this equation for some scalar `λ` (lambda).\n",
    "\n",
    "> $ A v = \\lambda v $  \n",
    "\n",
    "* **v:** The **eigenvector** (a non-zero vector).\n",
    "* **λ:** The **eigenvalue** (a scalar).\n",
    "\n",
    "This equation states that multiplying the matrix `A` by its eigenvector `v` has the exact same effect as just multiplying the vector `v` by a single number, its eigenvalue `λ`.\n",
    "\n",
    "### The Computational Advantage and a Practical Caveat\n",
    "\n",
    "The power of this equation is that it allows us to replace an \"expensive\" matrix multiplication with a \"cheap\" scalar multiplication, but only for vectors that are eigenvectors.\n",
    "\n",
    "If we can express any vector as a linear combination of the eigenvectors (using the **eigenbasis**), we can transform it without any matrix multiplication at all.\n",
    "\n",
    "For example, our vector $v_3 = (-1, 2)$ can be written as a combination of the eigenbasis:  \n",
    "\n",
    "$ v_3 = -3v_1 + 2v_2 $  \n",
    "\n",
    "To find its transformation, we can do:  \n",
    "\n",
    "$ A \\cdot v_3 = A \\cdot (-3v_1 + 2v_2) $  \n",
    "\n",
    "$ = -3(A \\cdot v_1) + 2(A \\cdot v_2) $  \n",
    "\n",
    "Now we use our shortcut, replacing $A \\cdot v$ with $\\lambda v$:  \n",
    "\n",
    "$ = -3(\\lambda_1 v_1) + 2(\\lambda_2 v_2) $  \n",
    "\n",
    "$ = -6\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 6\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 6 \\end{bmatrix} $  \n",
    "\n",
    "This gives us the correct answer using only scalar multiplication.\n",
    "\n",
    "**However, there is a crucial caveat.** Finding the coordinates of a vector with respect to a new basis (in this case, finding the numbers -3 and 2) is a significant calculation in itself. It requires finding the inverse of the eigenbasis matrix and multiplying it by our vector. This is often just as much work as the original matrix multiplication.\n",
    "\n",
    "The real advantage is not that eigenvectors remove all the work, but that they let you **decide when you want to do the work**. In some machine learning applications, it's worth paying the upfront cost to change all your data into the eigenbasis coordinate system, because you might need to apply the transformation many times later, and each of those subsequent transformations will be much faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
