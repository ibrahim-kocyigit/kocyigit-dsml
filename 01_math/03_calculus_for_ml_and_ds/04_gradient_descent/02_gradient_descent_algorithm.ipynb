{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b036d2d5-5768-44fa-bf24-2ad98bdca52b",
   "metadata": {},
   "source": [
    "# The Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb25f21-7b2a-49e7-b713-1ef1587c5ebc",
   "metadata": {},
   "source": [
    "In the previous lesson, we used a simple \"guess and check\" method to find the minimum of a function. It was inefficient because we had to test both the left and right directions at every step.\n",
    "\n",
    "There is a much smarter way. Instead of guessing, we can use the **derivative** to tell us the correct direction to move in.\n",
    "\n",
    "* If we are at a point where the slope of the tangent line is **negative**, we know the function is decreasing. To get to the minimum, we need to move to the **right** (increase our `x` value).\n",
    "* If we are at a point where the slope is **positive**, the function is increasing. We need to move to the **left** (decrease our `x` value).\n",
    "\n",
    "This leads to a simple rule: to get closer to the minimum, we should always move in the **opposite** direction of the slope.\n",
    "\n",
    "---\n",
    "## The Gradient Descent Formula\n",
    "\n",
    "This rule is captured in the core formula for **Gradient Descent**:\n",
    "\n",
    "$$ x_{new} = x_{old} - \\alpha \\cdot f'(x_{old}) $$\n",
    "\n",
    "Let's break this down:\n",
    "* $x_{new}$: Our new, improved position that is closer to the minimum.  \n",
    "\n",
    "* $x_{old}$: Our current position.  \n",
    "\n",
    "* $f'(x_{old})$: The derivative (slope) at our current position.  \n",
    "\n",
    "* $\\alpha$ (alpha): The **learning rate**. This is a small positive number (e.g., 0.01) that controls how big of a step we take.\n",
    "\n",
    "**Why it works:**\n",
    "* If the slope `f'(x)` is positive, we subtract a positive number, and `x` decreases (moves left).\n",
    "* If the slope `f'(x)` is negative, we subtract a negative number (which is addition), and `x` increases (moves right).\n",
    "\n",
    "**The Learning Rate (`α`)**\n",
    "The learning rate is a crucial hyperparameter. If it's too large, our steps might be too big, and we could overshoot the minimum and bounce around chaotically. If it's too small, our steps will be tiny, and it could take a very long time to reach the minimum. \n",
    "\n",
    "A great feature of this formula is that it naturally takes smaller steps as it gets closer to the minimum, because the slope itself gets smaller (flatter) near the bottom. This is like playing golf: you take a big swing when you're far away, but a small, precise tap when you're close to the hole.\n",
    "\n",
    "![](./images/0201.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94871c5c-a364-41bc-823e-6dae69e5b798",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Gradient Descent Algorithm in Summary\n",
    "\n",
    "**Goal:** Find the minimum of a function `f(x)`.\n",
    "\n",
    "1.  **Define a learning rate (`α`)** and choose a random **starting point (`x₀`)**.  \n",
    "\n",
    "2.  **Calculate the derivative** `f'(x)` at your current point.  \n",
    "\n",
    "3.  **Update your position** using the formula: $x_{new} = x_{old} - \\alpha \\cdot f'(x_{old})$.  \n",
    "\n",
    "4.  **Repeat** step 2 and 3 for a fixed number of iterations, or until your position stops changing significantly.\n",
    "\n",
    "The most powerful advantage of this method is that we **never had to solve the equation $f'(x)=0$**. We only need to be able to *calculate* the value of the derivative at any given point. This makes it possible to optimize incredibly complex functions where finding an analytical solution would be impossible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
