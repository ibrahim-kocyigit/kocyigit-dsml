{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a0dab8-71de-4a59-90b3-b9acf0b8a6c9",
   "metadata": {},
   "source": [
    "# Gradient Descent on a 2D Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387ae23-7ef7-466e-83c2-35c732e548f6",
   "metadata": {},
   "source": [
    "In the previous lesson, we solved the 2D power line problem by finding an exact, **analytical solution**. We took the partial derivatives of the cost function, set them to zero, and solved a system of linear equations to find the optimal slope `m` and intercept `b`.\n",
    "\n",
    "While this works for simple problems, it becomes very difficult for models with many features. Now, we will solve the **exact same problem** using an iterative method: **Gradient Descent**.\n",
    "\n",
    "**The Problem Recap:**\n",
    "Find the line `y = mx + b` that minimizes the total cost (sum of squared vertical distances) for the three power line locations: (1, 2), (2, 5), and (3, 3).\n",
    "\n",
    "**The Cost Function (E):**\n",
    "$$ E(m, b) = 14m^2 + 3b^2 + 38 + 12mb - 42m - 20b $$\n",
    "\n",
    "**The Gradient (∇E):**\n",
    "The gradient of our cost function is the vector of its two partial derivatives:\n",
    "$$ \\nabla E(m, b) = \\begin{bmatrix} \\frac{\\partial E}{\\partial m} \\\\ \\frac{\\partial E}{\\partial b} \\end{bmatrix} = \\begin{bmatrix} 28m + 12b - 42 \\\\ 12m + 6b - 20 \\end{bmatrix} $$\n",
    "\n",
    "Our goal is to use this gradient to iteratively \"walk\" from a random starting point `(m₀, b₀)` down to the point where the cost `E` is at its minimum.\n",
    "\n",
    "![](./images/0501.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec6890-4d3b-4b9a-9e78-4d3f78cfd608",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Gradient Descent Algorithm\n",
    "\n",
    "The process is exactly the same as in the one-variable case, just applied to vectors.\n",
    "\n",
    "1.  **Start** with a random point `(m₀, b₀)`.\n",
    "2.  **Calculate the gradient vector** `∇E` at the current point.\n",
    "3.  **Update the point** by taking a small step in the opposite direction of the gradient:\n",
    "    $$ \\begin{bmatrix} m_{new} \\\\ b_{new} \\end{bmatrix} = \\begin{bmatrix} m_{old} \\\\ b_{old} \\end{bmatrix} - \\alpha \\cdot \\nabla E(m_{old}, b_{old}) $$\n",
    "4.  **Repeat** for many iterations.\n",
    "\n",
    "As you can see in the plot, the algorithm iteratively \"walks\" down the hill of the cost surface and converges to the same optimal solution we found analytically: `m = 0.5` and `b ≈ 2.33`.\n",
    "\n",
    "This iterative approach is far more scalable and is the standard method used to train most machine learning models, including very complex ones like neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
