{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d031b2-9d28-429c-b9de-b80d5253a18f",
   "metadata": {},
   "source": [
    "# Drawbacks of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e0255-cc9b-4862-b4c8-384201527a8d",
   "metadata": {},
   "source": [
    "While Gradient Descent is a powerful and fundamental optimization algorithm, it's not without its challenges. Successfully using it requires navigating two key problems: choosing an appropriate learning rate and dealing with the possibility of local minima.\n",
    "\n",
    "---\n",
    "## Challenge 1: The Difficulty of Finding the Optimal Learning Rate\n",
    "\n",
    "The **learning rate (`Î±`)** is a hyperparameter that controls the size of the steps the algorithm takes at each iteration. Finding a good learning rate is a \"Goldilocks\" problem: it must be \"just right.\"\n",
    "\n",
    "* **If the learning rate is too large:** The steps can be too big, causing the algorithm to overshoot the minimum and bounce around chaotically, never finding the best solution.\n",
    "* **If the learning rate is too small:** The steps will be tiny and inefficient. It might take an extremely long time to reach the minimum, or the process might stop before it gets there.\n",
    "\n",
    "Finding a good learning rate is a well-known research problem, and while there are advanced methods to adapt it during training, there is no single definitive way to find the perfect value.\n",
    "\n",
    "![](./images/0301.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150c2fe-656d-4703-bd4c-c593d5e6ce83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge 2: The Danger of Local Minima\n",
    "\n",
    "Gradient Descent is designed to find the bottom of the \"valley\" it starts in. However, some functions have many valleys.\n",
    "\n",
    "If we start our algorithm in the wrong place, it might confidently walk to the bottom of a small, nearby valley and get stuck there, without ever realizing there is a much deeper valley (the true minimum) somewhere else.\n",
    "\n",
    "* **Local Minimum:** A point that is lower than its immediate neighbors, but not the lowest point overall.\n",
    "* **Global Minimum:** The single lowest point across the entire function.\n",
    "\n",
    "**How do we overcome this?**\n",
    "There is no guaranteed way to find the global minimum. However, a common and effective strategy is to **run the gradient descent algorithm multiple times from many different random starting points.** The hope is that at least one of these starting points will be in the \"valley\" that contains the global minimum.\n",
    "\n",
    "![](./images/0302.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
