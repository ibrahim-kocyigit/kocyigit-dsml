{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa43839-3443-4ea7-aac1-8a81346847b2",
   "metadata": {},
   "source": [
    "# Gradient Descent for Multiple Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441bccc-0070-4f79-83c0-325713a0005b",
   "metadata": {},
   "source": [
    "In the previous lesson, we used Gradient Descent to find the optimal slope (`m`) and intercept (`b`) that minimized our cost function. The key idea was visualizing the cost as a 3D surface and \"walking\" to the bottom.\n",
    "\n",
    "Let's make this connection more explicit. Every possible line `y = mx + b` that we can draw through our data corresponds to a single point `(m, b)` in a \"parameter space.\" The cost (or error) of that line is the \"height\" above that point.\n",
    "\n",
    "* A **bad line** that is far from the data points has a **high cost** and corresponds to a **high point** on the cost surface.\n",
    "* The **best-fit line** has the **lowest cost** and corresponds to the **lowest point** (the minimum) on the cost surface.\n",
    "\n",
    "The process of fitting a line is the same as finding the minimum on this surface. Gradient Descent is the algorithm that takes us from a random, bad line to the best-fit line by iteratively walking down the cost surface.\n",
    "\n",
    "![](./images/0601.png)\n",
    "\n",
    "![](./images/0602.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952b35f-8443-4159-9aa9-e59dbe6151a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The General Case: Linear Regression\n",
    "\n",
    "Let's apply this to a more general problem. Imagine you have a dataset with *n* observations of TV advertising budget (`x`) and the corresponding product sales (`y`). The goal is to find the line of best fit.\n",
    "\n",
    "The cost for a single data point $(x_i, y_i)$ is the squared vertical distance to the line:\n",
    "$$ \\text{Error}_i = (y_i - (mx_i + b))^2 $$\n",
    "\n",
    "To get the total cost for the entire dataset, we take the **average** of the errors for all *n* points. This function is called the **Mean Squared Error (MSE)** and is the standard cost function for linear regression.\n",
    "\n",
    "$$ L(m, b) = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - (mx_i + b))^2 $$\n",
    "\n",
    "_Note: You will often see two variations of this formula in textbooks and tutorials._\n",
    "\n",
    "_1.  ***Using `1/(2n)` instead of `1/n`***: The extra `2` in the denominator is a common mathematical convenience. When we take the derivative of the cost function, the Power Rule brings down the exponent `2` from the squared term. This `2` then perfectly cancels out with the `1/2` at the front, which makes the final derivative formula cleaner and simpler to work with. This scaling does not change the location of the minimum, so both formulas lead to the same final model._\n",
    "\n",
    "_2.  ***Order of Subtraction***: The term inside the square is sometimes written as `(mxᵢ + b - yᵢ)` instead of `(yᵢ - (mxᵢ + b))`. This makes no difference to the final result because the difference is squared. Since `(5)²` and `(-5)²` are both `25`, the order of subtraction does not affect the calculated cost._\n",
    "\n",
    "![](./images/0603.png)\n",
    "\n",
    "![](./images/0604.png)\n",
    "\n",
    "---\n",
    "## The Gradient Descent Algorithm for Linear Regression\n",
    "\n",
    "1.  **Start** with a random line (i.e., choose random initial values for `m` and `b`).\n",
    "2.  **Iterate:**\n",
    "    * Calculate the gradient of the loss function `L(m, b)`.\n",
    "    * Update `m` and `b` by taking a small step in the opposite direction of the gradient.\n",
    "    * This update results in a new, slightly better line.\n",
    "3.  **Repeat** for many iterations until the line converges to the best possible fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d6277-8d27-42d8-9b21-0838d403fba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
