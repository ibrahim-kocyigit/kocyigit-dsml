{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47421e8f-a744-4721-88eb-bc56bbe44ae4",
   "metadata": {},
   "source": [
    "# Optimization of Log-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23bb5f-6edf-4108-a0ed-2cab4b34907f",
   "metadata": {},
   "source": [
    "In the previous lessons, our optimization example led to the **squared error** cost function, which is fundamental to regression. Now, we will explore another incredibly important function in machine learning called the **log-loss**, which is central to classification.\n",
    "\n",
    "We will build our intuition for log-loss using a classic example from statistics: coin flips.\n",
    "\n",
    "---\n",
    "\n",
    "## The Biased Coin Game\n",
    "\n",
    "Imagine a game where you toss a coin 10 times. You win a large prize only if you get a very specific sequence: **seven heads followed by three tails** (`HHHHHHHTTT`).\n",
    "\n",
    "You get to choose the coin you use for the game, and you have three options, each with a different bias:\n",
    "\n",
    "* **Coin 1:** 70% chance of Heads (p=0.7), 30% chance of Tails.\n",
    "* **Coin 2:** 50% chance of Heads (p=0.5), 50% chance of Tails (a fair coin).\n",
    "* **Coin 3:** 30% chance of Heads (p=0.3), 70% chance of Tails.\n",
    "\n",
    "**Problem:** Which coin would you choose to maximize your chances of winning?\n",
    "\n",
    "---\n",
    "\n",
    "## Calculating the Probabilities\n",
    "\n",
    "To pick the best coin, we need to calculate the probability of winning the game with each one. Since the coin flips are independent events, we can multiply their probabilities.\n",
    "\n",
    "* **For Coin 1 (p=0.7):**\n",
    "    * Probability = $(0.7)^7 \\times (0.3)^3 \\approx 0.00222$\n",
    "* **For Coin 2 (p=0.5):**\n",
    "    * Probability = $(0.5)^7 \\times (0.5)^3 = (0.5)^{10} \\approx 0.00098$\n",
    "* **For Coin 3 (p=0.3):**\n",
    "    * Probability = $(0.3)^7 \\times (0.7)^3 \\approx 0.000075$\n",
    "\n",
    "As we can see, **Coin 1** gives us the highest probability of winning.\n",
    "\n",
    "![](./images/10.png)\n",
    "\n",
    "---\n",
    "## Finding the *Optimal* Coin with Calculus\n",
    "\n",
    "But is a 70% bias the absolute best possible coin? Or could a 69% or 71% coin be even better? To find the perfect coin, we need to use calculus.\n",
    "\n",
    "Let `p` be the unknown probability of getting heads. The probability of getting tails is then `1 - p`.\n",
    "\n",
    "The probability of winning, as a function of `p`, is:\n",
    "$$ g(p) = p^7 (1-p)^3 $$\n",
    "\n",
    "Our goal is to find the value of `p` that **maximizes** this function. We do this by taking the derivative and setting it to zero.\n",
    "\n",
    "![](./images/11.png)\n",
    "\n",
    "As we see above, setting this to zero and solving for `p` is algebraically complex. This leads us to a much more elegant solution.\n",
    "\n",
    "---\n",
    "## The Logarithm Trick\n",
    "\n",
    "A very common and powerful trick in machine learning is to optimize the **logarithm** of the probability instead of the probability itself. This is because the logarithm function is \"monotonic\"â€”if the probability `g(p)` is at its maximum, then `log(g(p))` will also be at its maximum. Maximizing one is the same as maximizing the other.\n",
    "\n",
    "Let's define a new function, `G(p)`, as the natural logarithm of `g(p)`:\n",
    "$$ G(p) = \\ln(g(p)) = \\ln(p^7 (1-p)^3) $$\n",
    "\n",
    "Using the properties of logarithms, we can simplify this expression dramatically:\n",
    "* $\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$\n",
    "* $\\ln(a^k) = k \\cdot \\ln(a)$\n",
    "\n",
    "Applying these rules, we get:\n",
    "$$ G(p) = \\ln(p^7) + \\ln((1-p)^3) $$\n",
    "$$ G(p) = 7\\ln(p) + 3\\ln(1-p) $$\n",
    "\n",
    "This new function is much easier to differentiate.\n",
    "\n",
    "---\n",
    "## Optimizing the Log-Probability\n",
    "\n",
    "Now, let's take the derivative of our simplified function `G(p)` and set it to zero.\n",
    "\n",
    "$$ G'(p) = \\frac{d}{dp}(7\\ln(p) + 3\\ln(1-p)) $$\n",
    "Recall that the derivative of $\\ln(x)$ is $1/x$. Using this and the chain rule:\n",
    "$$ G'(p) = 7 \\cdot \\frac{1}{p} + 3 \\cdot \\frac{1}{1-p} \\cdot (-1) $$\n",
    "$$ G'(p) = \\frac{7}{p} - \\frac{3}{1-p} $$\n",
    "\n",
    "Now, we set this derivative to zero to find the maximum:\n",
    "$$ \\frac{7}{p} - \\frac{3}{1-p} = 0 $$\n",
    "$$ \\frac{7}{p} = \\frac{3}{1-p} $$\n",
    "$$ 7(1-p) = 3p $$\n",
    "$$ 7 - 7p = 3p $$\n",
    "$$ 7 = 10p $$\n",
    "$$ p = 0.7 $$\n",
    "\n",
    "Calculus confirms that the coin with a **70%** probability of heads is indeed the optimal coin.\n",
    "\n",
    "---\n",
    "\n",
    "## The Log-Loss Function\n",
    "\n",
    "This function we optimized is directly related to the **log-loss** function. In machine learning, it's conventional to work with cost functions that we want to **minimize**. Because the logarithm of a probability (a number between 0 and 1) is always negative, we often work with the **negative log-probability**.\n",
    "\n",
    "> The **log-loss** is the negative of the log-probability. Minimizing the log-loss is the same as maximizing the probability.\n",
    "\n",
    "This is a very useful cost function for classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "## Maximum Likelihood\n",
    "If we look closely, what we did was exactly machine learning. We found the best **model** to explain our **dataset**.\n",
    "\n",
    "* **The Dataset:** The 10 coin flips (`HHHHHHHTTT`).\n",
    "* **The Model:** A biased coin with an unknown probability `p` of landing on heads.\n",
    "\n",
    "We found the model that most likely fits our dataset by maximizing the probability function. This process is called **Maximum Likelihood Estimation**.\n",
    "\n",
    "---\n",
    "\n",
    "## Two Reasons to Use Log-Loss\n",
    "\n",
    "The question is, why did we use the logarithm of the probability? Why not just take the derivative of the original probability function? There are two critical reasons.\n",
    "\n",
    "### Reason 1: Derivatives of Products are Hard\n",
    "\n",
    "Our original probability function was a product:\n",
    "$$ g(p) = p^7 (1-p)^3 $$\n",
    "\n",
    "Taking the derivative of a product of two terms is manageable with the product rule. But what if our dataset was larger, for example, 6 heads and 4 tails, but in a mixed order like `HHTHTHHTTT`?\n",
    "\n",
    "The probability function would be a much more complicated product:\n",
    "$$ g(p) = p \\cdot p \\cdot (1-p) \\cdot p \\cdot (1-p) \\cdot p \\cdot p \\cdot (1-p) \\cdot (1-p) \\cdot (1-p) $$\n",
    "\n",
    "Taking the derivative of a product with 10 terms requires iterating the product rule, which becomes incredibly messy.\n",
    "\n",
    "However, if we take the logarithm first, the product becomes a simple sum:\n",
    "$$ G(p) = \\ln(g(p)) = 6\\ln(p) + 4\\ln(1-p) $$\n",
    "\n",
    "The derivative of this sum is trivial to calculate. While the result contains fractions (like `1/p`), this is a very small price to pay to avoid the complex derivative of a long product.\n",
    "\n",
    "### Reason 2: Products of Tiny Numbers are a Computational Problem\n",
    "\n",
    "Probabilities are numbers between 0 and 1. When you multiply many of them together, the result can become astronomically small.\n",
    "\n",
    "Imagine we have a dataset with 1,000 data points. The total probability would be the product of 1,000 small numbers. The result might be a number so close to zero (e.g., $10^{-300}$) that a computer cannot store it accurately. This is called **numerical underflow**.\n",
    "\n",
    "Logarithms solve this problem elegantly. The logarithm of a very small number is a large negative number, which computers can handle with high precision.\n",
    "\n",
    "* `log(0.000000001)` = `-20.7`\n",
    "\n",
    "By converting our probabilities to the log domain, we turn a multiplication of tiny numbers into a sum of manageable negative numbers, which prevents numerical errors.\n",
    "\n",
    "> **Key Takeaway:** Any time you encounter a problem in machine learning that involves a very complicated product (especially with probabilities), your first instinct should be to take the logarithm. It simplifies the math and improves numerical stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
