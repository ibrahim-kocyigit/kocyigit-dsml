{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18040d50-667c-4dd4-92cb-33ad4c9ca869",
   "metadata": {},
   "source": [
    "# Classification with a Perceptron: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b994d-90f6-4c0f-923f-18b3e77870ab",
   "metadata": {},
   "source": [
    "Now that we have all the components of a classification perceptron, we can learn how to train it. The process is very similar to how we trained our regression model: we will use **Gradient Descent** to find the best possible weights and bias.\n",
    "\n",
    "Let's use our alien sentiment analysis problem as the example.\n",
    "\n",
    "**The Goal:**\n",
    "We want to find the perfect weights (`w₁`, `w₂`) and bias (`b`) that will make our perceptron's predictions (`ŷ`) match the true labels (`y`) as closely as possible. To do this, we must minimize an error, which we measure with a **loss function**.\n",
    "\n",
    "### The Loss Function for Classification: Log-Loss\n",
    "\n",
    "For regression, we used the Mean Squared Error loss function. For classification, while MSE *can* work, a much more effective and standard choice is the **Log-Loss** (also called Binary Cross-Entropy).\n",
    "\n",
    "We've already developed the intuition for this function in the \"Biased Coin Game\" example. It's derived from the principles of probability and measures how \"surprised\" the model is by the correct answer.\n",
    "\n",
    "The formula for the log-loss for a single data point is:\n",
    "$$ L(y, \\hat{y}) = -[y \\cdot \\ln(\\hat{y}) + (1-y) \\cdot \\ln(1-\\hat{y})] $$\n",
    "\n",
    "Let's break down how it works:\n",
    "* **If the true label `y` is 1:** The second part of the equation becomes zero. The loss is simply `-ln(ŷ)`. To make this loss small, the model needs to make `ŷ` (the predicted probability of being class 1) as close to 1 as possible.\n",
    "* **If the true label `y` is 0:** The first part of the equation becomes zero. The loss is `-ln(1-ŷ)`. To make this loss small, the model needs to make `ŷ` as close to 0 as possible.\n",
    "\n",
    "In short, the log-loss function heavily penalizes a model that is confidently wrong.\n",
    "\n",
    "---\n",
    "## Finding the Best Weights with Gradient Descent\n",
    "\n",
    "Our main goal is to find the weights `w₁`, `w₂`, and bias `b` that minimize the total log-loss across our entire dataset. We will use Gradient Descent to do this.\n",
    "\n",
    "The update rules are the same as before, but now we are taking the partial derivatives of our new log-loss function, `L`:\n",
    "\n",
    "* $ w_{1, new} = w_{1, old} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_1} $  \n",
    "\n",
    "* $ w_{2, new} = w_{2, old} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_2} $  \n",
    "\n",
    "* $ b_{new} = b_{old} - \\alpha \\cdot \\frac{\\partial L}{\\partial b} $\n",
    "\n",
    "![](./images/0601.png)\n",
    "\n",
    "The algorithm is as follows:\n",
    "1.  Start with random values for `w₁`, `w₂`, and `b`.\n",
    "2.  Calculate the partial derivatives of the log-loss with respect to each parameter.\n",
    "3.  Update the parameters by taking a small step in the opposite direction of the gradient.\n",
    "4.  Repeat for many iterations.\n",
    "\n",
    "In the next lesson, we will dive into the calculus and use the chain rule to find these partial derivatives. You will see that the sigmoid and log-loss functions work together beautifully to produce a very simple and elegant result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
