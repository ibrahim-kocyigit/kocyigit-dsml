{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2214f20-e77c-46dc-9def-b3fc93a0dbc6",
   "metadata": {},
   "source": [
    "# Classification with a Neural Network: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f4529-760e-4c3b-bf37-43fa13c23bb5",
   "metadata": {},
   "source": [
    "Now that we know what a perceptron is, we are just one step from understanding a **neural network**. A neural network is simply a collection of perceptrons organized in layers, where the outputs of one layer become the inputs for the next.\n",
    "\n",
    "### Why Do We Need More Than One Perceptron?\n",
    "\n",
    "A single perceptron, as we've seen, can only create a single straight line as its decision boundary. This is great for \"linearly separable\" problems, but many real-world relationships are more complex.\n",
    "\n",
    "To create more complex, non-linear decision boundaries, we need to combine multiple perceptrons. This is the core idea of a neural network.\n",
    "\n",
    "---\n",
    "## The Architecture of a Simple Neural Network\n",
    "\n",
    "Let's build a simple network with two layers. It will have one \"hidden layer\" with two perceptrons, and an \"output layer\" with one perceptron that gives us the final prediction.\n",
    "\n",
    "![](./images/0801.png)\n",
    "\n",
    "1.  **Input Layer:** Our features, $x_1$ and $x_2$.\n",
    "2.  **Hidden Layer:** This layer contains two perceptrons (the red and green nodes). Each one takes the original inputs, applies its own unique set of weights and a bias, and calculates its own output ($a_1$ and $a_2$).\n",
    "3.  **Output Layer:** This layer contains one final perceptron (the purple node). It takes the outputs of the hidden layer ($a_1$ and $a_2$) as its inputs, applies *its* own weights and bias, and produces the final prediction, `ŷ`.\n",
    "\n",
    "By combining perceptrons in this way, the network can learn much more complex patterns than a single straight line.\n",
    "\n",
    "---\n",
    "## The Math of the Neural Network\n",
    "\n",
    "Let's break down the calculations step-by-step.\n",
    "\n",
    "### The Hidden Layer\n",
    "\n",
    "* **Red Perceptron (Node 1):**\n",
    "    * Summation: $z_1 = w_{11}x_1 + w_{21}x_2 + b_1$\n",
    "    * Activation: $a_1 = \\sigma(z_1)$  \n",
    "` `\n",
    "* **Green Perceptron (Node 2):**\n",
    "    * Summation: $z_2 = w_{12}x_1 + w_{22}x_2 + b_2$\n",
    "    * Activation: $a_2 = \\sigma(z_2)$\n",
    "\n",
    "The values $a_1$ and $a_2$ are the outputs of the hidden layer.\n",
    "\n",
    "### The Output Layer\n",
    "\n",
    "* **Purple Perceptron (Output Node):**\n",
    "    * This perceptron takes $a_1$ and $a_2$ as its inputs.\n",
    "    * Summation: $z = w_1a_1 + w_2a_2 + b$\n",
    "    * Final Activation (Prediction): $\\hat{y} = \\sigma(z)$\n",
    "\n",
    "This final value, `ŷ`, is the network's prediction—a probability between 0 and 1.\n",
    "\n",
    "---\n",
    "## Training the Neural Network\n",
    "\n",
    "To train this network, our goal is the same as before: we need to find the optimal values for **all** the weights and biases (`w₁₁`, `w₂₁`, `b₁`, `w₁₂`, `w₂₂`, `b₂`, `w₁`, `w₂`, `b`) that minimize our **log-loss function**.\n",
    "\n",
    "We will do this using **Gradient Descent**. The process is conceptually the same, but now we must calculate the partial derivative of the loss with respect to *every single parameter* in the network. This requires repeated application of the **chain rule**, a process famously known as **backpropagation**, which we will explore in the next lesson.\n",
    "\n",
    "![](./images/0802.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
