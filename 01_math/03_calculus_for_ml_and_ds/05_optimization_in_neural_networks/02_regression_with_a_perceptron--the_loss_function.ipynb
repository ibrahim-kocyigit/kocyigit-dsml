{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bfd206-5195-47c0-923d-e7802f430696",
   "metadata": {},
   "source": [
    "# Regression with a Perceptron: The Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0214ead-10e3-4fd3-92ce-9f4b482b577c",
   "metadata": {},
   "source": [
    "In the previous lesson, we saw how a perceptron can represent a linear regression model. The model is the line itself, defined by its weights (`w`) and bias (`b`).\n",
    "\n",
    "The key question is: **How do we find the *best possible* line?**\n",
    "\n",
    "To do this, we need a way to mathematically evaluate how well our line is doing. We need to quantify how far our model's predictions are from the actual values. This is the job of a **loss function**.\n",
    "\n",
    "### The Error (or Residual)\n",
    "For any single data point, the **error** (also called the residual) is the vertical distance between the actual value (`y`) and the value our line predicted (`Å·`).\n",
    "\n",
    "$$ \\text{Error}_i = y_i - \\hat{y}_i $$\n",
    "\n",
    "Some of these errors will be positive (if the point is above the line) and some will be negative (if the point is below the line). If we just added them up, they might cancel each other out, giving us a misleadingly small total error.\n",
    "\n",
    "![](./images/0201.png)\n",
    "\n",
    "### The Squared Error\n",
    "To solve this, we **square** each individual error. This has two benefits:\n",
    "1.  All errors become positive.\n",
    "2.  It penalizes larger errors much more heavily than smaller ones.\n",
    "\n",
    "The standard loss function for linear regression is the **Mean Squared Error (MSE)**. For now, we will look at the loss for a single point, often written as:\n",
    "\n",
    "$$ L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2 $$\n",
    "\n",
    "*(Note: The `1/2` is a mathematical convenience that makes the derivative cleaner. It doesn't change the location of the minimum.)*\n",
    "\n",
    "Our ultimate goal is to find the weights `w` and bias `b` that **minimize** the sum of this loss function across all the data points in our dataset. We will achieve this using **Gradient Descent**.\n",
    "\n",
    "![](./images/0202.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
