{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a463d8-8d6f-42d8-bbb6-27a272f5d9ac",
   "metadata": {},
   "source": [
    "# Regression with a Perceptron: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb6921-b606-475d-9c17-7a68b16d8b45",
   "metadata": {},
   "source": [
    "Now we know our goal: find the optimal weights (`w₁`, `w₂`) and bias (`b`) that minimize our **loss function**, $L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$. In other words, we want to find the model that makes the smallest mistakes.\n",
    "\n",
    "To do this, we will use **Gradient Descent**. The algorithm starts with random values for the weights and bias and iteratively updates them by taking small steps in the direction that most steeply decreases the loss.\n",
    "\n",
    "The update rules for our three parameters are:\n",
    "* $ w_{1, new} = w_{1, old} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_1} $  \n",
    "\n",
    "* $ w_{2, new} = w_{2, old} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_2} $  \n",
    "\n",
    "* $ b_{new} = b_{old} - \\alpha \\cdot \\frac{\\partial L}{\\partial b} $\n",
    "\n",
    "To use these formulas, we first need to calculate the three partial derivatives of the loss function.\n",
    "\n",
    "---\n",
    "## Breaking Down the Derivatives with the Chain Rule\n",
    "\n",
    "This looks complicated, but we can simplify it using the **chain rule**. The loss `L` is not directly a function of the weights and bias. Instead, `L` depends on the prediction `ŷ`, which in turn depends on the weights and bias.\n",
    "\n",
    "This creates a chain of dependencies:\n",
    "$$ w_1, w_2, b \\quad \\longrightarrow \\quad \\hat{y} \\quad \\longrightarrow \\quad L $$\n",
    "\n",
    "We can use the chain rule to find the derivatives we need:\n",
    "* $ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w_1} $  \n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w_2} $  \n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial b} $\n",
    "\n",
    "Now, our problem is much simpler. We just need to calculate the four individual component derivatives.\n",
    "\n",
    "---\n",
    "## Calculating the Component Derivatives\n",
    "\n",
    "**Reference Formulas:**\n",
    "* **Loss Function:** $ L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2 $\n",
    "* **Prediction Function:** $ \\hat{y} = w_1x_1 + w_2x_2 + b $\n",
    "\n",
    "**1. Derivative of Loss with respect to Prediction ($\\frac{\\partial L}{\\partial \\hat{y}}$):**\n",
    "Using the chain rule, the derivative of $\\frac{1}{2}(\\text{something})^2$ is just the \"something,\" multiplied by the derivative of the inside with respect to `ŷ`.\n",
    "$$ \\frac{\\partial L}{\\partial \\hat{y}} = \\frac{1}{2} \\cdot 2(y - \\hat{y}) \\cdot (-1) = -(y - \\hat{y}) $$\n",
    "\n",
    "**2. Derivative of Prediction with respect to Bias ($\\frac{\\partial \\hat{y}}{\\partial b}$):**\n",
    "When differentiating with respect to `b`, the terms `w₁x₁` and `w₂x₂` are treated as constants, so their derivative is zero.\n",
    "$$ \\frac{\\partial \\hat{y}}{\\partial b} = 0 + 0 + 1 = 1 $$\n",
    "\n",
    "**3. Derivative of Prediction with respect to Weight 1 ($\\frac{\\partial \\hat{y}}{\\partial w_1}$):**\n",
    "When differentiating with respect to `w₁`, the term `w₂x₂ + b` is a constant. The derivative of `w₁x₁` with respect to `w₁` is just `x₁`.\n",
    "$$ \\frac{\\partial \\hat{y}}{\\partial w_1} = x_1 $$\n",
    "\n",
    "**4. Derivative of Prediction with respect to Weight 2 ($\\frac{\\partial \\hat{y}}{\\partial w_2}$):**\n",
    "Similarly, the derivative with respect to `w₂` is `x₂`.\n",
    "$$ \\frac{\\partial \\hat{y}}{\\partial w_2} = x_2 $$\n",
    "\n",
    "---\n",
    "## Assembling the Final Gradient\n",
    "\n",
    "Now we can plug these simple components back into our chain rule formulas.\n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial b} = -(y - \\hat{y}) \\cdot 1 = -(y - \\hat{y}) $  \n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w_1} = -(y - \\hat{y}) \\cdot x_1 $  \n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w_2} = -(y - \\hat{y}) \\cdot x_2 $\n",
    "\n",
    "These are the three partial derivatives that form our gradient.\n",
    "\n",
    "---\n",
    "## The Final Gradient Descent Step\n",
    "\n",
    "We can now write out the complete update rules for a single step of gradient descent for linear regression.\n",
    "\n",
    "* $ w_1 \\leftarrow w_1 - \\alpha \\cdot (-(y - \\hat{y}) \\cdot x_1) = w_1 + \\alpha (y - \\hat{y}) x_1 $  \n",
    "\n",
    "* $ w_2 \\leftarrow w_2 - \\alpha \\cdot (-(y - \\hat{y}) \\cdot x_2) = w_2 + \\alpha (y - \\hat{y}) x_2 $  \n",
    "\n",
    "* $ b \\leftarrow b - \\alpha \\cdot (-(y - \\hat{y})) = b + \\alpha (y - \\hat{y}) $\n",
    "\n",
    "By repeating these update steps many times for all the points in our dataset, the algorithm will find the optimal weights `w₁`, `w₂`, and bias `b` that result in the smallest possible error and therefore the best possible model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
