{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18040d50-667c-4dd4-92cb-33ad4c9ca869",
   "metadata": {},
   "source": [
    "# Classification with a Perceptron: The Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b994d-90f6-4c0f-923f-18b3e77870ab",
   "metadata": {},
   "source": [
    "In the previous video, we learned that to turn a regression perceptron into a classification perceptron, we need to add an **activation function**. The job of this function is to take the continuous output of the summation step (`z`) and convert it into a value that represents a probability, typically between 0 and 1.\n",
    "\n",
    "The most common activation function for binary classification is the **sigmoid function**.\n",
    "\n",
    "**Formula:**\n",
    "The sigmoid function, denoted as $\\sigma(z)$, is defined by the formula:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "**Key Property:**\n",
    "The sigmoid function takes the entire number line as its input and \"squashes\" it into the interval `(0, 1)`.\n",
    "* If `z` is a large positive number, $e^{-z}$ is very close to 0, so $\\sigma(z)$ is close to 1.\n",
    "* If `z` is a large negative number, $e^{-z}$ is very large, so $\\sigma(z)$ is close to 0.\n",
    "* If `z` is exactly 0, $\\sigma(0) = \\frac{1}{1+e^0} = \\frac{1}{1+1} = \\frac{1}{2}$.\n",
    "\n",
    "This property is perfect for classification, as the output can be interpreted as a probability.\n",
    "\n",
    "![](./images/0501.png)\n",
    "\n",
    "---\n",
    "\n",
    "## The Derivative of the Sigmoid Function\n",
    "\n",
    "Another reason the sigmoid function is so popular in machine learning is that its derivative is very simple to calculate and has an elegant form. This is crucial for training neural networks using gradient descent, as it makes the calculations much more efficient.\n",
    "\n",
    "Let's find the derivative, $\\sigma'(z)$.\n",
    "\n",
    "1.  **Rewrite the function** using a negative exponent:\n",
    "    $$ \\sigma(z) = (1 + e^{-z})^{-1} $$\n",
    "\n",
    "2.  **Apply the chain rule:** The derivative of $(\\text{something})^{-1}$ is $-1 \\cdot (\\text{something})^{-2}$, multiplied by the derivative of the \"something.\"\n",
    "    $$ \\sigma'(z) = -1 \\cdot (1 + e^{-z})^{-2} \\cdot \\frac{d}{dz}(1 + e^{-z}) $$\n",
    "\n",
    "3.  **Calculate the inner derivative:** The derivative of $1$ is $0$, and the derivative of $e^{-z}$ is $e^{-z} \\cdot (-1)$.\n",
    "    $$ \\sigma'(z) = -(1 + e^{-z})^{-2} \\cdot (-e^{-z}) $$\n",
    "\n",
    "4.  **Simplify:** The two negative signs cancel out.\n",
    "    $$ \\sigma'(z) = \\frac{e^{-z}}{(1 + e^{-z})^2} $$\n",
    "\n",
    "5.  **A clever algebraic trick:** We can rewrite the numerator by adding and subtracting 1: $e^{-z} = (1 + e^{-z}) - 1$.\n",
    "    $$ \\sigma'(z) = \\frac{(1 + e^{-z}) - 1}{(1 + e^{-z})^2} $$\n",
    "\n",
    "6.  **Split the fraction:**\n",
    "    $$ \\sigma'(z) = \\frac{1 + e^{-z}}{(1 + e^{-z})^2} - \\frac{1}{(1 + e^{-z})^2} $$\n",
    "    $$ = \\frac{1}{1 + e^{-z}} - \\left(\\frac{1}{1 + e^{-z}}\\right)^2 $$\n",
    "\n",
    "7.  **Recognize the sigmoid function:** The term $\\frac{1}{1 + e^{-z}}$ is just our original sigmoid function, $\\sigma(z)$.\n",
    "    $$ \\sigma'(z) = \\sigma(z) - (\\sigma(z))^2 $$\n",
    "\n",
    "> **The Final Rule:**\n",
    "> $$ \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) $$\n",
    "\n",
    "This beautiful result means that the derivative of the sigmoid at a point `z` can be calculated directly from its **output value**, without needing to refer back to `z` or calculate any more exponentials. This makes the backpropagation algorithm in neural networks much faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
