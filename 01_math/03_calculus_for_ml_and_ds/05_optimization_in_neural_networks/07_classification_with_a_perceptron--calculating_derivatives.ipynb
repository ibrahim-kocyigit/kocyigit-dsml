{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18040d50-667c-4dd4-92cb-33ad4c9ca869",
   "metadata": {},
   "source": [
    "# Classification with a Perceptron: Calculating Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b994d-90f6-4c0f-923f-18b3e77870ab",
   "metadata": {},
   "source": [
    "We now know that our goal is to find the optimal weights and bias for our classification perceptron by minimizing the **log-loss** function using **Gradient Descent**.\n",
    "\n",
    "To do this, we need to calculate the partial derivatives of the loss `L` with respect to each of our model's parameters: `w₁`, `w₂`, and `b`.\n",
    "\n",
    "Let's trace how a change in a single weight, like `w₁`, affects the final loss. It's not a direct relationship; there is a chain of dependencies.\n",
    "\n",
    "$$ w_1 \\quad \\longrightarrow \\quad z \\quad \\longrightarrow \\quad \\hat{y} \\quad \\longrightarrow \\quad L $$\n",
    "\n",
    "* A change in `w₁` affects the summation `z`.\n",
    "* A change in `z` affects the sigmoid output `ŷ`.\n",
    "* A change in `ŷ` affects the final loss `L`.\n",
    "\n",
    "To find the overall effect of `w₁` on `L`, we can use the **chain rule**.\n",
    "\n",
    "---\n",
    "## Breaking Down the Derivatives with the Chain Rule\n",
    "\n",
    "We can break down our complex problem into smaller, simpler derivatives:\n",
    "* $ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_1} $  \n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_2} $  \n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} $\n",
    "\n",
    "Our task is to find each of these individual components and multiply them together.\n",
    "\n",
    "---\n",
    "## Calculating the Component Derivatives\n",
    "\n",
    "**Reference Formulas:**\n",
    "* **Log-Loss:** $ L(y, \\hat{y}) = -[y \\ln(\\hat{y}) + (1-y) \\ln(1-\\hat{y})] $\n",
    "* **Prediction:** $ \\hat{y} = \\sigma(z) $\n",
    "* **Summation:** $ z = w_1x_1 + w_2x_2 + b $\n",
    "\n",
    "**1. Derivative of Loss w.r.t. Prediction ($\\frac{\\partial L}{\\partial \\hat{y}}$):**\n",
    "Taking the derivative of the log-loss function with respect to `ŷ` gives:\n",
    "$$ \\frac{\\partial L}{\\partial \\hat{y}} = - \\left[ \\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}} \\right] = \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})} $$\n",
    "\n",
    "**2. Derivative of Prediction w.r.t. Summation ($\\frac{\\partial \\hat{y}}{\\partial z}$):**\n",
    "This is the derivative of the sigmoid function, which we already know has a beautiful form:\n",
    "$$ \\frac{\\partial \\hat{y}}{\\partial z} = \\sigma'(z) = \\sigma(z)(1-\\sigma(z)) = \\hat{y}(1-\\hat{y}) $$\n",
    "\n",
    "**3. Derivatives of Summation w.r.t. Parameters:**\n",
    "These are simple partial derivatives:\n",
    "* $ \\frac{\\partial z}{\\partial w_1} = x_1 $  \n",
    "\n",
    "* $ \\frac{\\partial z}{\\partial w_2} = x_2 $  \n",
    "\n",
    "* $ \\frac{\\partial z}{\\partial b} = 1 $\n",
    "\n",
    "---\n",
    "## Assembling the Final Gradient\n",
    "\n",
    "Now we can multiply our components together. Let's start with the derivative with respect to `w₁`.\n",
    "$$ \\frac{\\partial L}{\\partial w_1} = \\underbrace{\\left(\\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}\\right)}_{\\frac{\\partial L}{\\partial \\hat{y}}} \\cdot \\underbrace{(\\hat{y}(1-\\hat{y}))}_{\\frac{\\partial \\hat{y}}{\\partial z}} \\cdot \\underbrace{(x_1)}_{\\frac{\\partial z}{\\partial w_1}} $$\n",
    "\n",
    "Notice the beautiful cancellation! The denominator `ŷ(1-ŷ)` cancels out perfectly with the derivative of the sigmoid function.\n",
    "$$ \\frac{\\partial L}{\\partial w_1} = (\\hat{y}-y)x_1 $$\n",
    "\n",
    "The same cancellation happens for the other two parameters.\n",
    "\n",
    "**The Final Gradient Components:**\n",
    "* $ \\frac{\\partial L}{\\partial w_1} = (\\hat{y}-y)x_1 $  \n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial w_2} = (\\hat{y}-y)x_2 $  \n",
    "\n",
    "* $ \\frac{\\partial L}{\\partial b} = (\\hat{y}-y) $\n",
    "\n",
    "The term `(ŷ - y)` is simply the **error** in our prediction. The derivative is just the error, scaled by the corresponding input. If the error is small, the derivatives are small, and the weights don't need to change much.\n",
    "\n",
    "---\n",
    "## The Final Gradient Descent Step\n",
    "\n",
    "We can now write out the complete update rules for a single step of gradient descent for our classification perceptron.\n",
    "\n",
    "* $ w_1 \\leftarrow w_1 - \\alpha \\cdot ((\\hat{y}-y)x_1) $\n",
    "* $ w_2 \\leftarrow w_2 - \\alpha \\cdot ((\\hat{y}-y)x_2) $\n",
    "* $ b \\leftarrow b - \\alpha \\cdot (\\hat{y}-y) $\n",
    "\n",
    "By repeating these simple update steps, the algorithm will find the optimal weights and bias for our classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
