{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2214f20-e77c-46dc-9def-b3fc93a0dbc6",
   "metadata": {},
   "source": [
    "# Classification with a Neural Network: Minimizing Log-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f4529-760e-4c3b-bf37-43fa13c23bb5",
   "metadata": {},
   "source": [
    "Now that we have our neural network architecture, our goal is to find the optimal values for all its weights and biases to minimize the log-loss function. We will do this using **Gradient Descent**.\n",
    "\n",
    "The process requires us to calculate the partial derivative of the final loss `L` with respect to *every single parameter* in the network. This tells us how to adjust each weight and bias to reduce the overall error.\n",
    "\n",
    "This is where the **chain rule** becomes incredibly powerful.\n",
    "\n",
    "---\n",
    "## The Chain of Dependencies\n",
    "\n",
    "Let's trace how a single weight from the first layer, like `w₁₁`, affects the final loss `L`. It's a long chain:\n",
    "\n",
    "![](./images/0901.png)\n",
    "\n",
    "$$ w_{11} \\quad \\longrightarrow \\quad z_1 \\quad \\longrightarrow \\quad a_1 \\quad \\longrightarrow \\quad z \\quad \\longrightarrow \\quad \\hat{y} \\quad \\longrightarrow \\quad L $$\n",
    "\n",
    "To find the derivative $\\frac{\\partial L}{\\partial w_{11}}$, we must multiply the derivatives of each link in this chain:\n",
    "$$ \\frac{\\partial L}{\\partial w_{11}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} $$\n",
    "This process of propagating the error backward through the network's layers is called **backpropagation**.\n",
    "\n",
    "---\n",
    "## Calculating the Component Derivatives\n",
    "\n",
    "Let's break down the long chain into its simpler components.\n",
    "\n",
    "**Reference Formulas:**\n",
    "* **Loss:** $ L(y, \\hat{y}) = -[y \\ln(\\hat{y}) + (1-y) \\ln(1-\\hat{y})] $  \n",
    "\n",
    "* **Final Activation:** $ \\hat{y} = \\sigma(z) $  \n",
    "\n",
    "* **Final Summation:** $ z = w_1a_1 + w_2a_2 + b $  \n",
    "\n",
    "* **Hidden Activation:** $ a_1 = \\sigma(z_1) $  \n",
    "\n",
    "* **Hidden Summation:** $ z_1 = w_{11}x_1 + w_{21}x_2 + b_1 $\n",
    "\n",
    "**The Derivatives:**\n",
    "1.  **$\\frac{\\partial L}{\\partial \\hat{y}}$:** We already calculated this for the single perceptron. It is $\\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}$.  \n",
    "\n",
    "2.  **$\\frac{\\partial \\hat{y}}{\\partial z}$:** The derivative of the sigmoid function is $\\hat{y}(1-\\hat{y})$.  \n",
    "\n",
    "3.  **$\\frac{\\partial z}{\\partial a_1}$:** From the final summation, this is simply the weight $w_1$.  \n",
    "\n",
    "4.  **$\\frac{\\partial a_1}{\\partial z_1}$:** This is another sigmoid derivative, so it's $a_1(1-a_1)$.  \n",
    "\n",
    "5.  **$\\frac{\\partial z_1}{\\partial w_{11}}$:** From the hidden summation, this is simply the input $x_1$.  \n",
    "\n",
    "---\n",
    "## Assembling the Gradient for the First Layer\n",
    "\n",
    "Now, let's multiply these components together to find the derivative for a first-layer weight, $\\frac{\\partial L}{\\partial w_{11}}$:\n",
    "$$ \\frac{\\partial L}{\\partial w_{11}} = \\underbrace{\\left(\\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}\\right)} \\cdot \\underbrace{(\\hat{y}(1-\\hat{y}))} \\cdot \\underbrace{(w_1)} \\cdot \\underbrace{(a_1(1-a_1))} \\cdot \\underbrace{(x_1)} $$\n",
    "\n",
    "The first two terms cancel out beautifully, leaving a much simpler expression:\n",
    "$$ \\frac{\\partial L}{\\partial w_{11}} = (\\hat{y}-y) \\cdot w_1 \\cdot a_1(1-a_1) \\cdot x_1 $$\n",
    "Similarly, for the first-layer bias `b₁`, the only change is the last term:\n",
    "$$ \\frac{\\partial L}{\\partial b_1} = (\\hat{y}-y) \\cdot w_1 \\cdot a_1(1-a_1) \\cdot 1 $$\n",
    "\n",
    "---\n",
    "## Assembling the Gradient for the Second Layer\n",
    "\n",
    "The chain for the second-layer weights (like `w₁`) is much shorter:\n",
    "$$ w_1 \\quad \\longrightarrow \\quad z \\quad \\longrightarrow \\quad \\hat{y} \\quad \\longrightarrow \\quad L $$\n",
    "$$ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_1} $$\n",
    "$$ = \\underbrace{\\left(\\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}\\right)} \\cdot \\underbrace{(\\hat{y}(1-\\hat{y}))} \\cdot \\underbrace{(a_1)} = (\\hat{y}-y)a_1 $$\n",
    "\n",
    "---\n",
    "## Summary of Final Gradient Descent Update Rules\n",
    "\n",
    "After all the calculus and cancellations, we are left with surprisingly elegant update rules. The term `(ŷ - y)` is the final error of the network.\n",
    "\n",
    "**For the Output Layer (Purple Node):**\n",
    "* $ w_1 \\leftarrow w_1 - \\alpha \\cdot (\\hat{y}-y)a_1 $  \n",
    "\n",
    "* $ w_2 \\leftarrow w_2 - \\alpha \\cdot (\\hat{y}-y)a_2 $  \n",
    "\n",
    "* $ b \\leftarrow b - \\alpha \\cdot (\\hat{y}-y) $\n",
    "\n",
    "**For the Hidden Layer (Red Node):**\n",
    "* $ w_{11} \\leftarrow w_{11} - \\alpha \\cdot (\\hat{y}-y)w_1 a_1(1-a_1)x_1 $  \n",
    "\n",
    "* $ w_{21} \\leftarrow w_{21} - \\alpha \\cdot (\\hat{y}-y)w_1 a_1(1-a_1)x_2 $\n",
    "\n",
    "* $ b_1 \\leftarrow b_1 - \\alpha \\cdot (\\hat{y}-y)w_1 a_1(1-a_1) $\n",
    "\n",
    "*(Similar rules apply for the Green Node's parameters)*\n",
    "\n",
    "By iterating these update steps many times, we can train our neural network to find the optimal set of weights and biases that best fits our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
