{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a7f9db-5e6e-4160-bef4-faa936097d44",
   "metadata": {},
   "source": [
    "# Gradient Descent and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b485bfd-81c6-487c-a71a-a6862a8766ca",
   "metadata": {},
   "source": [
    "Now that we know how to calculate the derivatives for a single perceptron, we can extend this process to train a full, multi-layer neural network. The method is the same: we will use **Gradient Descent** to find the best weights and biases.\n",
    "\n",
    "The only difference is that now we have many more parameters, so we need a systematic way to calculate the derivative of the loss function with respect to every single one of them.\n",
    "\n",
    "---\n",
    "## A Deeper Neural Network Architecture\n",
    "\n",
    "Let's consider a neural network with three layers of transformations: an input layer, **two hidden layers**, and an output layer. To keep track of all the parameters, we will use a superscript to denote the layer number.\n",
    "\n",
    "![](./images/1001.png)\n",
    "\n",
    "* **Layer 1 (First Hidden Layer):**\n",
    "    * Weights: $W^{(1)}$, Biases: $b^{(1)}$\n",
    "    * Activations: $a^{(1)} = \\sigma(z^{(1)})$  \n",
    "` `\n",
    "* **Layer 2 (Second Hidden Layer):**\n",
    "    * Weights: $W^{(2)}$, Biases: $b^{(2)}$\n",
    "    * Activations: $a^{(2)} = \\sigma(z^{(2)})$  \n",
    "` `\n",
    "* **Layer 3 (Output Layer):**\n",
    "    * Weights: $W^{(3)}$, Bias: $b^{(3)}$\n",
    "    * Final Prediction: $\\hat{y} = a^{(3)} = \\sigma(z^{(3)})$  \n",
    "` `\n",
    "The goal is to adjust all the weights and biases in all three layers to minimize the **log-loss function**, $L(y, \\hat{y})$.\n",
    "\n",
    "---\n",
    "## Backpropagation: The Chain Rule on a Grand Scale\n",
    "\n",
    "To update each parameter using Gradient Descent, we need to find the partial derivative of the final loss `L` with respect to that parameter. This requires us to trace the path of influence from the parameter all the way to the final loss, applying the chain rule at each step.\n",
    "\n",
    "This process of calculating the gradients by propagating the error signal backward through the network's layers is called **backpropagation**.\n",
    "\n",
    "Let's trace the chain for a weight in the very first layer, like $w_{11}^{(1)}$. The chain of dependencies is now much longer:\n",
    "\n",
    "$$ w_{11}^{(1)} \\to z_1^{(1)} \\to a_1^{(1)} \\to z^{(2)} \\to a^{(2)} \\to z^{(3)} \\to \\hat{y} \\to L $$\n",
    "\n",
    "The chain rule for this is a very long product of derivatives:\n",
    "$$ \\frac{\\partial L}{\\partial w_{11}^{(1)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial a_1^{(1)}} \\cdot \\frac{\\partial a_1^{(1)}}{\\partial z_1^{(1)}} \\cdot \\frac{\\partial z_1^{(1)}}{\\partial w_{11}^{(1)}} $$\n",
    "\n",
    "---\n",
    "## The Efficiency of Backpropagation\n",
    "\n",
    "While this looks incredibly complex, the good news is that we don't have to calculate everything from scratch for each parameter. Backpropagation is a very efficient algorithm because it **reuses calculations**.\n",
    "\n",
    "The process works like this:\n",
    "1.  **Forward Pass:** We feed an input through the network and calculate the output `Å·` and the final loss `L`. We store all the intermediate values ($z^{(1)}, a^{(1)}, z^{(2)}, a^{(2)}, z^{(3)}$) along the way.  \n",
    "\n",
    "2.  **Backward Pass:**\n",
    "    * First, we calculate the derivatives at the very end of the chain ($\\frac{\\partial L}{\\partial \\hat{y}}$ and $\\frac{\\partial \\hat{y}}{\\partial z^{(3)}}$).\n",
    "    * We use these to find the gradients for the parameters in the **last layer** (Layer 3).\n",
    "    * Then, we take that result and continue \"propagating\" the error backward to calculate the gradients for the **second-to-last layer** (Layer 2), reusing the derivatives we've already computed.\n",
    "    * Finally, we continue this process back to the **first layer**, again reusing all the previous calculations.\n",
    "\n",
    "This step-by-step backward flow is much more efficient than calculating the entire long chain rule for every single weight individually.\n",
    "\n",
    "The good news for a machine learning practitioner is that modern libraries like TensorFlow and Keras perform this entire backpropagation process for you automatically. However, understanding that it's just a clever and recursive application of the chain rule is fundamental to knowing how neural networks truly learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
