{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18040d50-667c-4dd4-92cb-33ad4c9ca869",
   "metadata": {},
   "source": [
    "# Classification with a Perceptron: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b994d-90f6-4c0f-923f-18b3e77870ab",
   "metadata": {},
   "source": [
    "You've already seen how perceptrons can be used for **linear regression problems** to predict continuous values (like house prices). However, perceptrons are also fundamental to **classification problems**, where the goal is to predict discrete categories (like \"happy\" or \"sad\").\n",
    "\n",
    "The key to adapting a perceptron for classification is the addition of an **activation function**.\n",
    "\n",
    "Let's revisit an example you've seen before: determining the mood of alien sentences.\n",
    "Imagine we have collected four sentences from an alien civilization, and we've observed the mood of the alien when they spoke each one:\n",
    "\n",
    "* Sentence 1: Happy\n",
    "* Sentence 2: Sad\n",
    "* Sentence 3: Sad\n",
    "* Sentence 4: Happy\n",
    "\n",
    "To use this data in a model, we first need to convert the words into numbers. We'll count the occurrences of two key words: \"aack\" ($x_1$) and \"beep\" ($x_2$).\n",
    "\n",
    "| Sentence | Aack ($x_1$) | Beep ($x_2$) | Mood (Target `y`) |\n",
    "| :------- | :----------- | :----------- | :---------------- |\n",
    "| 1        | 3            | 0            | Happy (1)         |\n",
    "| 2        | 0            | 2            | Sad (0)           |\n",
    "| 3        | 1            | 3            | Sad (0)           |\n",
    "| 4        | 2            | 1            | Happy (1)         |\n",
    "\n",
    "*(Note: We've assigned 'Happy' to 1 and 'Sad' to 0, which is common for binary classification.)*\n",
    "\n",
    "If we plot these points, we might see a pattern:\n",
    "\n",
    "![](./images/0401.png)\n",
    "\n",
    "As you can see, the happy sentences tend to be on the bottom-right, and the sad sentences on the top-left. Our model's job is to find a way to separate these two categories.\n",
    "\n",
    "---\n",
    "## The Classification Perceptron\n",
    "\n",
    "The core structure of a classification perceptron is very similar to the regression perceptron, but with one crucial addition.\n",
    "\n",
    "Let's review its components:\n",
    "\n",
    "* **Inputs ($x_1, x_2$):** The numerical features (word counts).\n",
    "* **Weights ($w_1, w_2$):** Determine the importance of each word for predicting mood.\n",
    "    * If 'aack' correlates with happiness, $w_1$ will be positive.\n",
    "    * If 'beep' correlates with sadness, $w_2$ will be negative.\n",
    "* **Bias ($b$):** A constant term that helps shift the decision boundary.\n",
    "\n",
    "**The Summation (z):**\n",
    "Just like in regression, we first calculate a weighted sum of inputs plus the bias:\n",
    "$$ z = w_1x_1 + w_2x_2 + b $$\n",
    "This value, `z`, can be any continuous number, from very negative to very positive.\n",
    "\n",
    "**The Problem:** For classification, we don't want a continuous number as an output. We want a probability or a clear category (0 or 1). How do we turn a continuous number `z` into something that represents a probability between 0 and 1?\n",
    "\n",
    "![](./images/0402.png)\n",
    "\n",
    "**The Solution: The Activation Function**\n",
    "This is where the **activation function** comes in. For binary classification, a very common choice is the **sigmoid function**.\n",
    "\n",
    "The sigmoid function takes any real-valued number (`z`) and \"squashes\" it into a value between 0 and 1.\n",
    "$$ \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "* If `z` is a large positive number, $\\hat{y}$ will be close to 1.\n",
    "* If `z` is a large negative number, $\\hat{y}$ will be close to 0.\n",
    "* If `z` is 0, $\\hat{y}$ will be 0.5.\n",
    "\n",
    "**Interpreting the Output ($\\hat{y}$):**\n",
    "The output $\\hat{y}$ can be interpreted as the **probability** that the sentence belongs to the \"happy\" class (class 1).\n",
    "* $\\hat{y} = 0.9$: Model is confident it's happy.\n",
    "* $\\hat{y} = 0.1$: Model is confident it's sad.\n",
    "* $\\hat{y} = 0.5$: Model is uncertain (on the decision boundary).\n",
    "\n",
    "In the next section, we will delve deeper into the properties of the sigmoid function. For now, understand that it's the critical component that transforms the continuous output of the linear combination into a probability for classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
