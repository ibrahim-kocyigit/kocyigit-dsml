{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaec9d34-7c0d-4597-925f-30cd45a7ecca",
   "metadata": {},
   "source": [
    "# Newton's Method: An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa1250-72fa-463e-a07d-b58fa57910b8",
   "metadata": {},
   "source": [
    "Now that we know the theory, let's put Newton's method into action. We will use it to solve the optimization problem from our Gradient Descent lesson, which was to find the minimum of the function:\n",
    "$$ g(x) = e^x - \\ln(x) $$\n",
    "\n",
    "Recall that finding the minimum of a function `g(x)` is the same as finding the **root (or zero)** of its derivative, `g'(x)`.\n",
    "\n",
    "So, our new goal is to use Newton's method to find the root of the function:\n",
    "$$ f(x) = g'(x) = e^x - \\frac{1}{x} $$\n",
    "\n",
    "To do this, we need both this function `f(x)` and its derivative `f'(x)`, which is the second derivative of our original function, `g''(x)`.\n",
    "* **Function (g'):** $f(x) = e^x - \\frac{1}{x}$\n",
    "* **Derivative of Function (g''):** $f'(x) = e^x + \\frac{1}{x^2}$\n",
    "\n",
    "We will now apply the Newton's method update rule to iteratively find the root of `f(x)`.\n",
    "$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$\n",
    "\n",
    "Let's perform the first few iterations by hand to see the process in detail.\n",
    "\n",
    "* **Starting Point:** Let's choose $x_0 = 2.0$.\n",
    "* **Update Rule:** $ x_{new} = x_{old} - \\frac{e^{x_{old}} - 1/x_{old}}{e^{x_{old}} + 1/x_{old}^2} $\n",
    "\n",
    "### Iteration 1 (Finding x₁):\n",
    "1.  **Calculate f(x₀):** $f(2.0) = e^2 - 1/2 \\approx 7.389 - 0.5 = 6.889$\n",
    "2.  **Calculate f'(x₀):** $f'(2.0) = e^2 + 1/2^2 \\approx 7.389 + 0.25 = 7.639$\n",
    "3.  **Update x:** $x_1 = 2.0 - \\frac{6.889}{7.639} \\approx 2.0 - 0.902 = 1.098$\n",
    "\n",
    "### Iteration 2 (Finding x₂):\n",
    "1.  **Calculate f(x₁):** $f(1.098) = e^{1.098} - 1/1.098 \\approx 2.997 - 0.911 = 2.086$\n",
    "2.  **Calculate f'(x₁):** $f'(1.098) = e^{1.098} + 1/(1.098)^2 \\approx 2.997 + 0.830 = 3.827$\n",
    "3.  **Update x:** $x_2 = 1.098 - \\frac{2.086}{3.827} \\approx 1.098 - 0.545 = 0.553$\n",
    "\n",
    "In just two steps, we have already moved from `x=2.0` to `x=0.553`, which is very close to the true root. This demonstrates the rapid convergence of Newton's method.\n",
    "\n",
    "![](./images/0201.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f173472-0ec0-4387-a9f9-f47b29240649",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analyzing the Convergence\n",
    "\n",
    "As the plot shows, Newton's method converges to the solution with incredible speed.\n",
    "\n",
    "* **x₀ = 2.0**\n",
    "* **x₁ ≈ 0.97**\n",
    "* **x₂ ≈ 0.63**\n",
    "* **x₃ ≈ 0.570**\n",
    "* **x₄ ≈ 0.567**\n",
    "\n",
    "In only a few iterations, we have arrived at an excellent approximation of the true minimum (`x ≈ 0.567`), which is known as the Omega constant. This demonstrates the power and speed of Newton's method. Just like with Gradient Descent, we found the minimum without ever needing to solve the difficult equation $e^x - 1/x = 0$ analytically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
