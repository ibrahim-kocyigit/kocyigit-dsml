{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cda0cd-05fb-4846-85a9-437b1dbbdfb4",
   "metadata": {},
   "source": [
    "# The Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de4142-bcab-457f-a88a-7e3318710335",
   "metadata": {},
   "source": [
    "In the previous lesson, you learned about the second derivative and its usefulness in optimization. That was all for functions of one variable. Now we will see how this concept extends to functions of multiple variables.\n",
    "\n",
    "For multivariable functions, the second derivative is not a single number but a **matrix** full of second-order partial derivatives, called the **Hessian matrix**.\n",
    "\n",
    "Let's compare the progression from one to two variables:\n",
    "\n",
    "| Concept | One Variable | Two Variables |\n",
    "| :--- | :--- | :--- |\n",
    "| **Function** | $f(x)$ | $f(x, y)$ |\n",
    "| **1st Derivative**| $f'(x)$ (a scalar) | $\\nabla f = \\begin{bmatrix} f_x \\\\ f_y \\end{bmatrix}$ (a vector) |\n",
    "| **2nd Derivative**| $f''(x)$ (a scalar) | **The Hessian** (a matrix) |\n",
    "\n",
    "---\n",
    "## Calculating Second-Order Partial Derivatives\n",
    "\n",
    "To understand the Hessian, let's start with a function $f(x, y) = 2x^2 + 3y^2 - xy$.\n",
    "\n",
    "First, we find its first-order partial derivatives (the gradient):\n",
    "* $f_x = \\frac{\\partial f}{\\partial x} = 4x - y$  \n",
    "\n",
    "* $f_y = \\frac{\\partial f}{\\partial y} = 6y - x$\n",
    "\n",
    "Now, we can take the derivative of *each* of these results with respect to *each* of the variables again. This gives us four **second-order partial derivatives**:\n",
    "\n",
    "1.  **$f_{xx} = \\frac{\\partial}{\\partial x}(f_x) = \\frac{\\partial}{\\partial x}(4x - y) = 4$**  \n",
    "\n",
    "2.  **$f_{xy} = \\frac{\\partial}{\\partial y}(f_x) = \\frac{\\partial}{\\partial y}(4x - y) = -1$**  \n",
    "\n",
    "3.  **$f_{yx} = \\frac{\\partial}{\\partial x}(f_y) = \\frac{\\partial}{\\partial x}(6y - x) = -1$**  \n",
    "\n",
    "4.  **$f_{yy} = \\frac{\\partial}{\\partial y}(f_y) = \\frac{\\partial}{\\partial y}(6y - x) = 6$**\n",
    "\n",
    "Notice that the \"mixed\" partial derivatives, $f_{xy}$ and $f_{yx}$, are the same. This is almost always the case for the functions we encounter in machine learning.\n",
    "\n",
    "---\n",
    "## The Hessian Matrix\n",
    "\n",
    "The Hessian matrix, often denoted as `H` or `∇²f`, is the matrix that organizes all these second-order partial derivatives.\n",
    "\n",
    "For a function of two variables, the Hessian is a 2x2 matrix:\n",
    "$$ H = \\begin{bmatrix} f_{xx} & f_{xy} \\\\ f_{yx} & f_{yy} \\end{bmatrix} $$\n",
    "\n",
    "For our example function, $f(x, y) = 2x^2 + 3y^2 - xy$, the Hessian matrix is:\n",
    "$$ H = \\begin{bmatrix} 4 & -1 \\\\ -1 & 6 \\end{bmatrix} $$\n",
    "\n",
    "The Hessian matrix gives us a lot of information about the function's curvature at a given point and is very useful in advanced optimization methods, such as Newton's method for multiple variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
