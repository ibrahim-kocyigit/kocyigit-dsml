{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f47c1c-cf7d-415e-abc2-ad93560b20d8",
   "metadata": {},
   "source": [
    "# Newton's Method: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1cf15-1036-4e92-8ee9-035097f331dc",
   "metadata": {},
   "source": [
    "In the previous lessons, we learned how to find the minimum of a function using Gradient Descent. **Newton's method** is another powerful, iterative algorithm. However, its primary purpose is different: it's designed to find the **roots (or zeros)** of a function—the points where the function crosses the x-axis, i.e., where `f(x) = 0`.\n",
    "\n",
    "We can then cleverly adapt this root-finding algorithm for optimization.\n",
    "\n",
    "---\n",
    "## The Intuition: Riding the Tangent Line\n",
    "\n",
    "Newton's method is an iterative process for approximating a function's root. The core idea is:\n",
    "\n",
    "1.  **Start** at a random point `x₀` on the curve.\n",
    "2.  **Draw the tangent line** to the curve at that point.\n",
    "3.  **Find the root of the tangent line.** Follow the tangent line down until it crosses the x-axis. This crossing point is our new, better guess, `x₁`.\n",
    "4.  **Repeat.** Draw a new tangent line at `x₁`, find where it crosses the x-axis to get `x₂`, and so on.\n",
    "\n",
    "As you can see in the visualization, this process converges on the true root of the function incredibly quickly:\n",
    "\n",
    "![](./images/0101.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042bf3e-9570-4b32-88f6-aefb44d6571a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deriving the Formula\n",
    "\n",
    "We can derive the update rule for Newton's method from the definition of the slope. The slope of the tangent line at a point `xₖ` is `f'(xₖ)`.\n",
    "\n",
    "Using the rise-over-run formula for the tangent line:\n",
    "$$ \\text{Slope} = f'(x_k) = \\frac{\\text{Rise}}{\\text{Run}} = \\frac{f(x_k) - 0}{x_k - x_{k+1}} $$\n",
    "\n",
    "Now, we just need to solve this equation for our next point, $x_{k+1}$:\n",
    "$$ x_k - x_{k+1} = \\frac{f(x_k)}{f'(x_k)} $$\n",
    "$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$\n",
    "\n",
    "This is the iterative update rule for Newton's method.\n",
    "\n",
    "---\n",
    "## Using Newton's Method for Optimization\n",
    "\n",
    "How can we use a root-finding algorithm to find the **minimum** of a function?\n",
    "\n",
    "We simply remember the rule from our previous lessons: **the minimum of a function `g(x)` occurs where its derivative `g'(x)` is zero.**\n",
    "\n",
    "Therefore, to minimize `g(x)`, we can apply Newton's method to find the root of its derivative, `g'(x)`.\n",
    "\n",
    "This leads to a slightly different update rule for optimization. Let's compare the two:\n",
    "\n",
    "| Method | Goal | Update Rule |\n",
    "| :--- | :--- | :--- |\n",
    "| **Newton's Method (Root Finding)** | Find `x` such that `f(x) = 0` | $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$ |\n",
    "| **Newton's Method (Optimization)** | Find `x` such that `g'(x) = 0` | $x_{k+1} = x_k - \\frac{g'(x_k)}{g''(x_k)}$ |\n",
    "\n",
    "Notice that for optimization, we need to calculate both the first and the **second derivative** (`g''`) of our original function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
