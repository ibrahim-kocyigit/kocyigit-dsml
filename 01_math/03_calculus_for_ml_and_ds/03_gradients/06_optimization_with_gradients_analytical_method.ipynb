{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7174a88-d192-4f0b-8b27-2e29ecba7c16",
   "metadata": {},
   "source": [
    "# Optimization using Gradients: The Analytical Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b9277-5878-4344-b3f2-968daeb6188a",
   "metadata": {},
   "source": [
    "In this lesson, we'll use a more complex optimization example that introduces one of the most important models in machine learning: **linear regression**. We will use our knowledge of partial derivatives to find the \"line of best fit\" for a set of data points analytically.\n",
    "\n",
    "### The 2D Power Line Problem\n",
    "Imagine our power lines are now points on a 2D plane. We need to run a straight fiber optic cable (our \"line\") in a way that minimizes the cost of connecting each power line to it.\n",
    "\n",
    "* **Constraint:** The connection wires must run vertically.\n",
    "* **Cost:** The cost for each connection is the **square of the length** of that vertical wire.\n",
    "\n",
    "The goal is to find the optimal parameters for our line, `y = mx + b`—the **slope (m)** and **y-intercept (b)**—that minimize the total cost.\n",
    "\n",
    "![](./images/0601.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c04f89-56af-4941-8146-65b7bd41e27d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Formulating the Cost Function\n",
    "\n",
    "The vertical distance for any power line at $(x_i, y_i)$ to the line $y = mx + b$ is the difference in their y-values: $(y_i - (mx_i + b))$.\n",
    "\n",
    "The cost for that single point is this distance squared. The total cost, `E(m, b)`, is the sum of the costs for all three points.\n",
    "\n",
    "* **Cost for Point 1 (1, 2):** $(2 - (m(1) + b))^2 = (2 - m - b)^2$\n",
    "* **Cost for Point 2 (2, 5):** $(5 - (m(2) + b))^2 = (5 - 2m - b)^2$\n",
    "* **Cost for Point 3 (3, 3):** $(3 - (m(3) + b))^2 = (3 - 3m - b)^2$\n",
    "\n",
    "**Total Cost Function:**\n",
    "$$ E(m, b) = (2 - m - b)^2 + (5 - 2m - b)^2 + (3 - 3m - b)^2 $$\n",
    "\n",
    "After expanding and combining like terms, this polynomial becomes:\n",
    "$$ E(m, b) = 14m^2 + 3b^2 + 38 + 12mb - 42m - 20b $$\n",
    "\n",
    "This is the function we need to minimize. Our variables are now `m` and `b`.\n",
    "\n",
    "---\n",
    "## Optimization with Partial Derivatives\n",
    "\n",
    "To find the minimum of this function, we need to find the point where the gradient is the zero vector. This means we must calculate the partial derivatives with respect to `m` and `b` and set them both to zero.\n",
    "\n",
    "* **Partial derivative with respect to `m`:**\n",
    "  $$ \\frac{\\partial E}{\\partial m} = 28m + 12b - 42 $$\n",
    "\n",
    "* **Partial derivative with respect to `b`:**\n",
    "  $$ \\frac{\\partial E}{\\partial b} = 6b + 12m - 20 $$\n",
    "\n",
    "Now, we set both equations to zero to find the optimal `m` and `b`:\n",
    "1.  $28m + 12b - 42 = 0$  \n",
    "\n",
    "2.  $12m + 6b - 20 = 0$\n",
    "\n",
    "This is a 2x2 system of linear equations, which we can solve.\n",
    "1.  From the second equation, we can isolate `b`: $6b = 20 - 12m \\implies b = \\frac{20-12m}{6}$.  \n",
    "\n",
    "2.  Substitute this into the first equation: $28m + 12(\\frac{20-12m}{6}) - 42 = 0$.  \n",
    "\n",
    "3.  $28m + 2(20-12m) - 42 = 0 \\implies 28m + 40 - 24m - 42 = 0$.  \n",
    "\n",
    "4.  $4m - 2 = 0 \\implies \\boldsymbol{m = 0.5}$.  \n",
    "\n",
    "5.  Substitute `m=0.5` back into the equation for `b`: $b = \\frac{20-12(0.5)}{6} = \\frac{20-6}{6} = \\frac{14}{6} \\approx \\boldsymbol{2.33}$.\n",
    "\n",
    "The optimal line that minimizes the sum of squared errors has a slope of **0.5** and a y-intercept of approximately **2.33**. This process is called **Linear Regression**.\n",
    "\n",
    "![](./images/0602.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69469565-a084-4e66-8b8a-65f355e1f0d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Problem with the Analytical Method\n",
    "\n",
    "Solving this system of equations was manageable for two variables (`m` and `b`). But what if our model had hundreds or thousands of features? We would have to solve a system with hundreds or thousands of variables, which is computationally very expensive and complex.\n",
    "\n",
    "This motivates the need for an alternative, iterative method to find the minimum of the cost function. This method is called **Gradient Descent**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
